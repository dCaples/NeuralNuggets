{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "426e2f4f96a4491fa22a9e785560c83a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/2120 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1919175373"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # load dataset\n",
    "\n",
    "# from datasets import load_dataset\n",
    "# from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "# tokenizer = ByteLevelBPETokenizer()\n",
    "# dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "\n",
    "# # Specify the split you want to save (e.g., \"train\", \"validation\", \"test\")\n",
    "# split = \"train\"\n",
    "\n",
    "# # Get the desired split from the dataset\n",
    "# subset = dataset[split]\n",
    "\n",
    "# # Save the subset to a text file\n",
    "# subset.to_csv(\"tinystories-train.txt\", sep=\"\\t\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mllmhacking\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/diegocaples/Code/ai/wandb/run-20240420_165551-0rq5ngnz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/llmhacking/tinystories/runs/0rq5ngnz' target=\"_blank\">woven-microwave-3</a></strong> to <a href='https://wandb.ai/llmhacking/tinystories' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/llmhacking/tinystories' target=\"_blank\">https://wandb.ai/llmhacking/tinystories</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/llmhacking/tinystories/runs/0rq5ngnz' target=\"_blank\">https://wandb.ai/llmhacking/tinystories/runs/0rq5ngnz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/llmhacking/tinystories/runs/0rq5ngnz?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7b17bbf7a060>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#----- imports --------\n",
    "import tqdm\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "import tokenizers\n",
    "\n",
    "\n",
    "device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.set_default_device(device)\n",
    "assert device == 'cuda', \"This notebook is not optimized for CPU\"\n",
    "\n",
    "config = {\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"eval_interval\": 300,\n",
    "    \"max_iters\": 3000, \n",
    "    \"H\": 16,\n",
    "    \"B\": 64,\n",
    "    \"T\": 64,\n",
    "    \"C\": 256,\n",
    "    \"feedforward_factor\": 3,\n",
    "    \"n_heads\": 6,\n",
    "    \"dropout\": 0.4,\n",
    "    \"l2_penalty\": 0.1,\n",
    "    \"n_layers\": 6,\n",
    "    \"tokenizer_vocab_size\": 2**15,\n",
    "    \"git_hash\": os.popen(\"git rev-parse HEAD\").read().strip()\n",
    "}\n",
    "\n",
    "# initial\n",
    "for k,v in config.items():\n",
    "    locals ()[k] = v\n",
    "\n",
    "\n",
    "# wandb.init(\n",
    "#     project = \"tinystories\",\n",
    "#     config = config\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('tinystories-train.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1916206969\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in lines:  20550005\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in lines: \", len(text.split('\\n')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text\n",
      "\"One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
      "\n",
      "Lily went to her mom and said, \"\"Mom, I found this needle. Can you share it with me and sew my shirt?\"\" Her mom smiled and said, \"\"Yes, Lily, we can share the needle and fix your shirt.\"\"\n",
      "\n",
      "Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.\"\n",
      "\"Once upon a time, there was a little car named Beep. Beep loved to go fast and play in the sun. Beep was a healthy car because he always had good fuel. Good fuel made Beep happy and strong.\n",
      "\n",
      "One day, Beep was driving in the park when he saw a big tree. The tree had many leaves that wer\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "[6137]\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "paths = ['tinystories-train.txt']\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer()\n",
    "\n",
    "tokenizer.train(files=paths, vocab_size=tokenizer_vocab_size, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])\n",
    "\n",
    "tokenizer.save_model('.', 'tiny-stories-bpe')\n",
    "\n",
    "\n",
    "\n",
    "enc = tokenizer.encode(\"She sells sea shells by the sea shore!\")\n",
    "tokenizer.decode(enc.ids)\n",
    "\n",
    "\n",
    "def encode(text):\n",
    "    return tokenizer.encode(text).ids\n",
    "def decode(encoded_text):\n",
    "    return tokenizer.decode(encoded_text)\n",
    "\n",
    "hello_encoded = encode(\"hello\")\n",
    "print(hello_encoded)\n",
    "print(decode(hello_encoded))\n",
    "vocab_size = tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"length of dataset in characters: \", len(text[:10000]))\n",
    "print(\"length of dataset in tokens: \", len(encode(text[:10000])))\n",
    "chars_per_token = len(text[:10000]) / len(encode(text[:10000]))\n",
    "print(\"characters per token: \", chars_per_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "torch.Size([344152])\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.dtype)\n",
    "print(data.size())\n",
    "print(data.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 676, 1201,   30,  203, 2347,  336, 2752,  807, 2307,   16,  679,  322,\n",
       "         621,   18,  203,  203, 1236,   30,  203, 2543,   16,  621,   18,  203,\n",
       "         203,  676, 1201,   30,  203,  570,  423,  400, 2058,  773, 1503,  292,\n",
       "         969,  532,  292, 2797,  561,   35,  203,  203, 1236,   30,  203, 3667,\n",
       "         499,  773,   18, 2058,  773,   18,  203,  203,  676, 1201,   30,  203,\n",
       "         676,   16,  293,  509, 3997], device='cuda:0')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:T+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_data[:T]\n",
    "y = train_data[1:T+1]\n",
    "for t in range(T):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    # print(\"when we see the text\", context, \"we predict the next character is\", target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(0, data.size(0) - T, (B,)) # 4 random locations we can sample from\n",
    "    x = torch.stack([data[i:i+T] for i in ix]) # random sequences\n",
    "    y = torch.stack([data[i+1:i+T+1] for i in ix]) # next character for each random sequence\n",
    "\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "for b in range(B):\n",
    "    for t in range(T): # for each of the characters in the sample\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    '''One Head of self-attention'''\n",
    "    def __init__(self, H):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(C, H, bias=False)\n",
    "        self.key = nn.Linear(C, H, bias=False)\n",
    "        self.value = nn.Linear(C, H, bias=False)\n",
    "        # self.output = nn.Linear(H, C, bias=False) # output matrix\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(T, T)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Query and Key matrices for the attention mechanism\n",
    "        # x: 8 tokens\n",
    "        # Q: 16 tall (arbitrary), 32 long channels\n",
    "        # K: 16 tall (arbitrary), 32 long channels\n",
    "\n",
    "        query_vectors = self.query(x)\n",
    "        key_vectors = self.key(x)\n",
    "\n",
    "\n",
    "        # Attention masking(so we can't look into the past):\n",
    "\n",
    "        tril = self.tril\n",
    "        wei = torch.zeros(T, T) \n",
    "        wei = wei.masked_fill(tril == 0, float('-inf')) # set the upper triangular to -inf\n",
    "        # xbow = wei @ x # apply the mask to the input, bag of words because simple avg.\n",
    "\n",
    "        # multiply the two to get the attention weights\n",
    "        attention_pattern = query_vectors @ key_vectors.transpose(-2, -1) # T, T\n",
    "        attention_pattern = attention_pattern / (H ** 0.5) # scale the attention pattern for numerical stability\n",
    "        attention_weights = F.softmax(attention_pattern + wei, dim=-1) # T, T (the row dimension is the query)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        value_vectors = self.value(x) # the direction we should go in the embedding space for each token (ie more blue) T, H\n",
    "\n",
    "        # apply the attention weights to the value vectors\n",
    "        context = attention_weights @ value_vectors # T, H\n",
    "\n",
    "        # project back into original space from value space\n",
    "        # return self.output(context)\n",
    "        return context\n",
    "\n",
    "x = torch.randn(B,T,C)\n",
    "head = Head(H)\n",
    "# head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    '''Multiple heads of self-attention'''\n",
    "    def __init__(self, H, C, n_heads): # H is head embedding space size, n_heads is number of heads\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(H) for _ in range(n_heads)])\n",
    "        self.combine_heads = nn.Linear(H*n_heads, C)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        x = self.combine_heads(x)  # T, C\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64, 16])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head = MultiHeadAttention(H, C, n_heads)\n",
    "head.heads[0].forward(x).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    '''Feed-forward neural network'''\n",
    "    def __init__(self, C):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(C, C * feedforward_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(C * feedforward_factor, C),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    '''Layer normalization'''\n",
    "    def __init__(self, C, use_affine=True):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(C)) if use_affine else None\n",
    "        self.beta = nn.Parameter(torch.zeros(C)) if use_affine else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        if self.gamma is not None and self.beta is not None:\n",
    "            return self.gamma * (x - mean) / (std + 1e-6) + self.beta\n",
    "        else:\n",
    "            return (x - mean) / (std + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    '''Transformer block'''\n",
    "    def __init__(self, H, C, n_heads):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(H, C, n_heads)\n",
    "        self.ff = FeedForward(C)\n",
    "        self.norm1 = LayerNorm(C, use_affine=True)\n",
    "        self.norm2 = LayerNorm(C, use_affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.norm1(x))\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64, 4096])\n",
      "tensor(8.7672, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-2.3841,  1.0758, -1.2788,  ...,  0.8275, -0.7848, -0.0043],\n",
       "          [-0.6179, -2.2806,  0.1570,  ...,  2.7975, -0.9502,  1.6256],\n",
       "          [-1.1456,  0.3364,  0.3847,  ...,  1.6403, -1.9459, -0.5270],\n",
       "          ...,\n",
       "          [-1.0044, -1.1411, -0.1971,  ...,  1.2776, -0.2699,  1.2927],\n",
       "          [-2.0422, -0.6010, -0.1549,  ...,  1.0831, -1.7448, -0.0553],\n",
       "          [-1.4777, -0.6368,  0.0028,  ...,  2.0669, -0.9934,  0.3146]]],\n",
       "        device='cuda:0', grad_fn=<ViewBackward0>),\n",
       " None)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, n_layers):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, C) \n",
    "        self.position_embedding_table = nn.Embedding(T, C)\n",
    "        self.lm_head = nn.Linear(C, vocab_size)\n",
    "        self.layers = nn.ModuleList([Block(H, C, n_heads) for _ in range(n_layers)])\n",
    "        self.block = nn.ModuleList([Block(H, C, n_heads)])\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        token_emb = self.token_embedding_table(idx) # batch_dim, sequence_dim, embedding_dim\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T))\n",
    "        x = token_emb + pos_emb # token identities and positions contained\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        logits = self.lm_head(x) # batch_dim, sequence_dim, vocab_size\n",
    "\n",
    "        batch_dim, sequence_dim, embedding_dim = logits.size()\n",
    "\n",
    "        # loss = F.cross_entropy(logits, targets) this won't work because we need 1d logits and 1d targets\n",
    "        # one-hot-vectors are a line in the x-dimension, so the shape of shape of the logits should be (-1, vocab_size).\n",
    "\n",
    "        if targets is None:\n",
    "            return logits, None\n",
    "        else:\n",
    "            # a list of all the predictions, reguardles of batch.\n",
    "            # xdim: probabilities of each character in the vocab (embedding_dim=vocab_size)\n",
    "            # ydim: all predictions for all batches flattened (batch_dim*sequence_dim)\n",
    "            logits_loss_view = logits.view(-1, vocab_size) \n",
    "            # targets loss view\n",
    "            # xdim: all targets for all batches flattened (batch_dim*sequence_dim)\n",
    "            # so this would be like, [1,4,5,1,2,3, ...]\n",
    "            # where each number is the correct next index of the one hot vector\n",
    "            targets_loss_view = targets.view(-1)\n",
    "            loss = F.cross_entropy(logits_loss_view, targets_loss_view)\n",
    "            return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx[:,-T:])\n",
    "            # get the predictions of the last token\n",
    "            last_token_logits = logits[:, -1, :] # all batches, last token, all probabilities\n",
    "            # softmax to get probabilities\n",
    "            probabilities = F.softmax(last_token_logits, dim=-1)\n",
    "            # sample from the probabilities\n",
    "            next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "            # add the new token to the idx tensor\n",
    "            idx = torch.cat((idx, next_token), dim=1)\n",
    "        return idx\n",
    "\n",
    "    \n",
    "\n",
    "model = GPT(n_layers)\n",
    "logits, loss = model(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_idx = torch.zeros(1, T).long()\n",
    "model.forward(idx=test_idx)\n",
    "# decode(model.generate(idx=test_idx, max_new_tokens=100)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (token_embedding_table): Embedding(4096, 192)\n",
       "  (position_embedding_table): Embedding(64, 192)\n",
       "  (lm_head): Linear(in_features=192, out_features=4096, bias=True)\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x Block(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (query): Linear(in_features=192, out_features=16, bias=False)\n",
       "            (key): Linear(in_features=192, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=192, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (combine_heads): Linear(in_features=96, out_features=192, bias=True)\n",
       "        (dropout): Dropout(p=0.4, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (3): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "    )\n",
       "  )\n",
       "  (block): ModuleList(\n",
       "    (0): Block(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (query): Linear(in_features=192, out_features=16, bias=False)\n",
       "            (key): Linear(in_features=192, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=192, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (combine_heads): Linear(in_features=96, out_features=192, bias=True)\n",
       "        (dropout): Dropout(p=0.4, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (3): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]], device='cuda:0')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logits, loss = self(idx[:,-T:])\n",
    "\n",
    "idx = torch.zeros(1, 1).long()\n",
    "idx[:,-T:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.token_embedding_table.weight.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_iters = 10\n",
    "eval_interval = 300\n",
    "@torch.no_grad()\n",
    "def estimate_loss(is_last=False):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        real_iters = eval_iters\n",
    "        if is_last and split == 'val':  # increase last eval to mitigate noise\n",
    "            real_iters *= 10 \n",
    "        losses = torch.zeros(real_iters)\n",
    "        for k in range(real_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean() / chars_per_token\n",
    "    model.train()\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter_to_data_ratio=13.505591858873363\n",
      "token_embedding_table.weight: 786432\n",
      "lm_head.weight: 786432\n",
      "layers.0.ff.net.0.weight: 147456\n",
      "layers.0.ff.net.2.weight: 147456\n",
      "layers.1.ff.net.0.weight: 147456\n",
      "layers.1.ff.net.2.weight: 147456\n",
      "layers.2.ff.net.0.weight: 147456\n",
      "layers.2.ff.net.2.weight: 147456\n",
      "layers.3.ff.net.0.weight: 147456\n",
      "layers.3.ff.net.2.weight: 147456\n",
      "layers.4.ff.net.0.weight: 147456\n",
      "layers.4.ff.net.2.weight: 147456\n",
      "layers.5.ff.net.0.weight: 147456\n",
      "layers.5.ff.net.2.weight: 147456\n",
      "block.0.ff.net.0.weight: 147456\n",
      "block.0.ff.net.2.weight: 147456\n",
      "layers.0.attention.combine_heads.weight: 18432\n",
      "layers.1.attention.combine_heads.weight: 18432\n",
      "layers.2.attention.combine_heads.weight: 18432\n",
      "layers.3.attention.combine_heads.weight: 18432\n",
      "layers.4.attention.combine_heads.weight: 18432\n",
      "layers.5.attention.combine_heads.weight: 18432\n",
      "block.0.attention.combine_heads.weight: 18432\n",
      "position_embedding_table.weight: 12288\n",
      "lm_head.bias: 4096\n",
      "layers.0.attention.heads.0.query.weight: 3072\n",
      "layers.0.attention.heads.0.key.weight: 3072\n",
      "layers.0.attention.heads.0.value.weight: 3072\n",
      "layers.0.attention.heads.1.query.weight: 3072\n",
      "layers.0.attention.heads.1.key.weight: 3072\n",
      "layers.0.attention.heads.1.value.weight: 3072\n",
      "layers.0.attention.heads.2.query.weight: 3072\n",
      "layers.0.attention.heads.2.key.weight: 3072\n",
      "layers.0.attention.heads.2.value.weight: 3072\n",
      "layers.0.attention.heads.3.query.weight: 3072\n",
      "layers.0.attention.heads.3.key.weight: 3072\n",
      "layers.0.attention.heads.3.value.weight: 3072\n",
      "layers.0.attention.heads.4.query.weight: 3072\n",
      "layers.0.attention.heads.4.key.weight: 3072\n",
      "layers.0.attention.heads.4.value.weight: 3072\n",
      "layers.0.attention.heads.5.query.weight: 3072\n",
      "layers.0.attention.heads.5.key.weight: 3072\n",
      "layers.0.attention.heads.5.value.weight: 3072\n",
      "layers.1.attention.heads.0.query.weight: 3072\n",
      "layers.1.attention.heads.0.key.weight: 3072\n",
      "layers.1.attention.heads.0.value.weight: 3072\n",
      "layers.1.attention.heads.1.query.weight: 3072\n",
      "layers.1.attention.heads.1.key.weight: 3072\n",
      "layers.1.attention.heads.1.value.weight: 3072\n",
      "layers.1.attention.heads.2.query.weight: 3072\n",
      "layers.1.attention.heads.2.key.weight: 3072\n",
      "layers.1.attention.heads.2.value.weight: 3072\n",
      "layers.1.attention.heads.3.query.weight: 3072\n",
      "layers.1.attention.heads.3.key.weight: 3072\n",
      "layers.1.attention.heads.3.value.weight: 3072\n",
      "layers.1.attention.heads.4.query.weight: 3072\n",
      "layers.1.attention.heads.4.key.weight: 3072\n",
      "layers.1.attention.heads.4.value.weight: 3072\n",
      "layers.1.attention.heads.5.query.weight: 3072\n",
      "layers.1.attention.heads.5.key.weight: 3072\n",
      "layers.1.attention.heads.5.value.weight: 3072\n",
      "layers.2.attention.heads.0.query.weight: 3072\n",
      "layers.2.attention.heads.0.key.weight: 3072\n",
      "layers.2.attention.heads.0.value.weight: 3072\n",
      "layers.2.attention.heads.1.query.weight: 3072\n",
      "layers.2.attention.heads.1.key.weight: 3072\n",
      "layers.2.attention.heads.1.value.weight: 3072\n",
      "layers.2.attention.heads.2.query.weight: 3072\n",
      "layers.2.attention.heads.2.key.weight: 3072\n",
      "layers.2.attention.heads.2.value.weight: 3072\n",
      "layers.2.attention.heads.3.query.weight: 3072\n",
      "layers.2.attention.heads.3.key.weight: 3072\n",
      "layers.2.attention.heads.3.value.weight: 3072\n",
      "layers.2.attention.heads.4.query.weight: 3072\n",
      "layers.2.attention.heads.4.key.weight: 3072\n",
      "layers.2.attention.heads.4.value.weight: 3072\n",
      "layers.2.attention.heads.5.query.weight: 3072\n",
      "layers.2.attention.heads.5.key.weight: 3072\n",
      "layers.2.attention.heads.5.value.weight: 3072\n",
      "layers.3.attention.heads.0.query.weight: 3072\n",
      "layers.3.attention.heads.0.key.weight: 3072\n",
      "layers.3.attention.heads.0.value.weight: 3072\n",
      "layers.3.attention.heads.1.query.weight: 3072\n",
      "layers.3.attention.heads.1.key.weight: 3072\n",
      "layers.3.attention.heads.1.value.weight: 3072\n",
      "layers.3.attention.heads.2.query.weight: 3072\n",
      "layers.3.attention.heads.2.key.weight: 3072\n",
      "layers.3.attention.heads.2.value.weight: 3072\n",
      "layers.3.attention.heads.3.query.weight: 3072\n",
      "layers.3.attention.heads.3.key.weight: 3072\n",
      "layers.3.attention.heads.3.value.weight: 3072\n",
      "layers.3.attention.heads.4.query.weight: 3072\n",
      "layers.3.attention.heads.4.key.weight: 3072\n",
      "layers.3.attention.heads.4.value.weight: 3072\n",
      "layers.3.attention.heads.5.query.weight: 3072\n",
      "layers.3.attention.heads.5.key.weight: 3072\n",
      "layers.3.attention.heads.5.value.weight: 3072\n",
      "layers.4.attention.heads.0.query.weight: 3072\n",
      "layers.4.attention.heads.0.key.weight: 3072\n",
      "layers.4.attention.heads.0.value.weight: 3072\n",
      "layers.4.attention.heads.1.query.weight: 3072\n",
      "layers.4.attention.heads.1.key.weight: 3072\n",
      "layers.4.attention.heads.1.value.weight: 3072\n",
      "layers.4.attention.heads.2.query.weight: 3072\n",
      "layers.4.attention.heads.2.key.weight: 3072\n",
      "layers.4.attention.heads.2.value.weight: 3072\n",
      "layers.4.attention.heads.3.query.weight: 3072\n",
      "layers.4.attention.heads.3.key.weight: 3072\n",
      "layers.4.attention.heads.3.value.weight: 3072\n",
      "layers.4.attention.heads.4.query.weight: 3072\n",
      "layers.4.attention.heads.4.key.weight: 3072\n",
      "layers.4.attention.heads.4.value.weight: 3072\n",
      "layers.4.attention.heads.5.query.weight: 3072\n",
      "layers.4.attention.heads.5.key.weight: 3072\n",
      "layers.4.attention.heads.5.value.weight: 3072\n",
      "layers.5.attention.heads.0.query.weight: 3072\n",
      "layers.5.attention.heads.0.key.weight: 3072\n",
      "layers.5.attention.heads.0.value.weight: 3072\n",
      "layers.5.attention.heads.1.query.weight: 3072\n",
      "layers.5.attention.heads.1.key.weight: 3072\n",
      "layers.5.attention.heads.1.value.weight: 3072\n",
      "layers.5.attention.heads.2.query.weight: 3072\n",
      "layers.5.attention.heads.2.key.weight: 3072\n",
      "layers.5.attention.heads.2.value.weight: 3072\n",
      "layers.5.attention.heads.3.query.weight: 3072\n",
      "layers.5.attention.heads.3.key.weight: 3072\n",
      "layers.5.attention.heads.3.value.weight: 3072\n",
      "layers.5.attention.heads.4.query.weight: 3072\n",
      "layers.5.attention.heads.4.key.weight: 3072\n",
      "layers.5.attention.heads.4.value.weight: 3072\n",
      "layers.5.attention.heads.5.query.weight: 3072\n",
      "layers.5.attention.heads.5.key.weight: 3072\n",
      "layers.5.attention.heads.5.value.weight: 3072\n",
      "block.0.attention.heads.0.query.weight: 3072\n",
      "block.0.attention.heads.0.key.weight: 3072\n",
      "block.0.attention.heads.0.value.weight: 3072\n",
      "block.0.attention.heads.1.query.weight: 3072\n",
      "block.0.attention.heads.1.key.weight: 3072\n",
      "block.0.attention.heads.1.value.weight: 3072\n",
      "block.0.attention.heads.2.query.weight: 3072\n",
      "block.0.attention.heads.2.key.weight: 3072\n",
      "block.0.attention.heads.2.value.weight: 3072\n",
      "block.0.attention.heads.3.query.weight: 3072\n",
      "block.0.attention.heads.3.key.weight: 3072\n",
      "block.0.attention.heads.3.value.weight: 3072\n",
      "block.0.attention.heads.4.query.weight: 3072\n",
      "block.0.attention.heads.4.key.weight: 3072\n",
      "block.0.attention.heads.4.value.weight: 3072\n",
      "block.0.attention.heads.5.query.weight: 3072\n",
      "block.0.attention.heads.5.key.weight: 3072\n",
      "block.0.attention.heads.5.value.weight: 3072\n",
      "layers.0.ff.net.0.bias: 768\n",
      "layers.1.ff.net.0.bias: 768\n",
      "layers.2.ff.net.0.bias: 768\n",
      "layers.3.ff.net.0.bias: 768\n",
      "layers.4.ff.net.0.bias: 768\n",
      "layers.5.ff.net.0.bias: 768\n",
      "block.0.ff.net.0.bias: 768\n",
      "layers.0.attention.combine_heads.bias: 192\n",
      "layers.0.ff.net.2.bias: 192\n",
      "layers.0.norm1.gamma: 192\n",
      "layers.0.norm1.beta: 192\n",
      "layers.0.norm2.gamma: 192\n",
      "layers.0.norm2.beta: 192\n",
      "layers.1.attention.combine_heads.bias: 192\n",
      "layers.1.ff.net.2.bias: 192\n",
      "layers.1.norm1.gamma: 192\n",
      "layers.1.norm1.beta: 192\n",
      "layers.1.norm2.gamma: 192\n",
      "layers.1.norm2.beta: 192\n",
      "layers.2.attention.combine_heads.bias: 192\n",
      "layers.2.ff.net.2.bias: 192\n",
      "layers.2.norm1.gamma: 192\n",
      "layers.2.norm1.beta: 192\n",
      "layers.2.norm2.gamma: 192\n",
      "layers.2.norm2.beta: 192\n",
      "layers.3.attention.combine_heads.bias: 192\n",
      "layers.3.ff.net.2.bias: 192\n",
      "layers.3.norm1.gamma: 192\n",
      "layers.3.norm1.beta: 192\n",
      "layers.3.norm2.gamma: 192\n",
      "layers.3.norm2.beta: 192\n",
      "layers.4.attention.combine_heads.bias: 192\n",
      "layers.4.ff.net.2.bias: 192\n",
      "layers.4.norm1.gamma: 192\n",
      "layers.4.norm1.beta: 192\n",
      "layers.4.norm2.gamma: 192\n",
      "layers.4.norm2.beta: 192\n",
      "layers.5.attention.combine_heads.bias: 192\n",
      "layers.5.ff.net.2.bias: 192\n",
      "layers.5.norm1.gamma: 192\n",
      "layers.5.norm1.beta: 192\n",
      "layers.5.norm2.gamma: 192\n",
      "layers.5.norm2.beta: 192\n",
      "block.0.attention.combine_heads.bias: 192\n",
      "block.0.ff.net.2.bias: 192\n",
      "block.0.norm1.gamma: 192\n",
      "block.0.norm1.beta: 192\n",
      "block.0.norm2.gamma: 192\n",
      "block.0.norm2.beta: 192\n"
     ]
    }
   ],
   "source": [
    "# get the number of parameters\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "parameter_to_data_ratio = n_params / len(train_data)\n",
    "print(f\"{parameter_to_data_ratio=}\")\n",
    "\n",
    "parameters = []\n",
    "for name, param in model.named_parameters():\n",
    "    parameters.append({\"name\": name, \"params\": param.numel()})\n",
    "\n",
    "# sort parameters by size\n",
    "sorted_parameters = sorted(parameters, key=lambda x: x[\"params\"], reverse=True)\n",
    "for p in sorted_parameters:\n",
    "    print(f\"{p['name']}: {p['params']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [01:16<00:00, 39.16it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fd6ea2717ec44b3af0a00612c8b5032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>l2</td><td>█▄▃▂▁▁▁▁▁▂</td></tr><tr><td>train</td><td>█▃▃▂▂▂▂▁▁▁▁</td></tr><tr><td>val</td><td>█▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>l2</td><td>0.17729</td></tr><tr><td>train</td><td>0.9381</td></tr><tr><td>val</td><td>1.43439</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">crimson-grass-16</strong> at: <a href='https://wandb.ai/llmhacking/mini-shakespeare/runs/rakly8ra' target=\"_blank\">https://wandb.ai/llmhacking/mini-shakespeare/runs/rakly8ra</a><br/> View project at: <a href='https://wandb.ai/llmhacking/mini-shakespeare' target=\"_blank\">https://wandb.ai/llmhacking/mini-shakespeare</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240420_160535-rakly8ra/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_params = sum([p.numel() for p in model.parameters()])\n",
    "\n",
    "for steps in tqdm.tqdm(range(max_iters)):\n",
    "    xb, yb = get_batch('train')\n",
    "    # loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # l2 regularization\n",
    "    l2 = sum(p.pow(2).sum() for p in model.parameters()) / num_params\n",
    "    loss = loss + l2 * l2_penalty\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        wandb.log({\"train\": losses['train'].item(), \"val\": losses['val'].item(), \"l2\":l2})\n",
    "\n",
    "losses = estimate_loss(is_last=True)\n",
    "wandb.log({\"train\": losses['train'].item(), \"val\": losses['val'].item()})\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': tensor(0.9418, device='cuda:0'),\n",
       " 'val': tensor(1.4320, device='cuda:0')}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my lord a precious queen\n",
      "To visiting me to bring me from this haste\n",
      "Than that my spring enrichised him said with such a battle\n",
      "Forget, in aptier'd the deputy of this:\n",
      "What valiant Richard doth he sends the thigh.\n",
      "Romeo I hand thy thoughts to permit to wonder the God\n",
      "That will win him.\n",
      "\n",
      "GLOUCESTER:\n",
      "What would theseBISHOP OF YORK:\n",
      "O me, cousin, the painful dog is here.\n",
      "I'll find out-night never spake thy wretched hay\n",
      "And spinking cloy's with heavy eyes;\n",
      "Bid much excuse thy false dearly doth beguem,\n",
      "Why should thrown me with batt the heart?\n",
      "And not in the golden fee with thy lips,\n",
      "Nor no tongue, love his life\n"
     ]
    }
   ],
   "source": [
    "test_idx = torch.zeros(1, T).long()\n",
    "print(decode(\n",
    "    model.generate(idx=test_idx, max_new_tokens=C)[0].tolist()\n",
    ")[T:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
