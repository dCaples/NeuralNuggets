{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----- imports --------\n",
    "import tqdm\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "import tokenizers\n",
    "\n",
    "\n",
    "device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.set_default_device(device)\n",
    "assert device == 'cuda', \"This notebook is not optimized for CPU\"\n",
    "\n",
    "config = {\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"eval_interval\": 300,\n",
    "    \"max_iters\": 3000, \n",
    "    \"H\": 16,\n",
    "    \"B\": 64,\n",
    "    \"T\": 128,\n",
    "    \"C\": 256,\n",
    "    \"feedforward_factor\": 3,\n",
    "    \"n_heads\": 6,\n",
    "    \"dropout\": 0.4,\n",
    "    \"l2_penalty\": 0.0,\n",
    "    \"n_layers\": 6,\n",
    "    \"tokenizer_vocab_size\": 4096,\n",
    "    \"git_hash\": os.popen(\"git rev-parse HEAD\").read().strip()\n",
    "}\n",
    "\n",
    "# initial\n",
    "for k,v in config.items():\n",
    "    locals ()[k] = v\n",
    "\n",
    "\n",
    "# wandb.init(\n",
    "#     project = \"mini-shakespeare\",\n",
    "#     config = config\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in lines:  40001\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in lines: \", len(text.split('\\n')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "[262, 278, 83]\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "paths = ['input.txt']\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer()\n",
    "\n",
    "tokenizer.train(files=paths, vocab_size=tokenizer_vocab_size, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])\n",
    "\n",
    "tokenizer.save_model('.', 'shakespeare-bpe')\n",
    "\n",
    "\n",
    "\n",
    "enc = tokenizer.encode(\"Romeo Romeo wherefore art thou Romeo?\")\n",
    "tokenizer.decode(enc.ids)\n",
    "\n",
    "\n",
    "def encode(text):\n",
    "    return tokenizer.encode(text).ids\n",
    "def decode(encoded_text):\n",
    "    return tokenizer.decode(encoded_text)\n",
    "\n",
    "hello_encoded = encode(\"hello\")\n",
    "print(hello_encoded)\n",
    "print(decode(hello_encoded))\n",
    "vocab_size = tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n",
      "length of dataset in tokens:  344152\n",
      "characters per token:  3.2409923522164625\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))\n",
    "print(\"length of dataset in tokens: \", len(encode(text)))\n",
    "chars_per_token = len(text) / len(encode(text))\n",
    "print(\"characters per token: \", chars_per_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "torch.Size([344152])\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.dtype)\n",
    "print(data.size())\n",
    "print(data.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 676, 1201,   30,  203, 2347,  336, 2752,  807, 2307,   16,  679,  322,\n",
       "         621,   18,  203,  203, 1236,   30,  203, 2543,   16,  621,   18,  203,\n",
       "         203,  676, 1201,   30,  203,  570,  423,  400, 2058,  773, 1503,  292,\n",
       "         969,  532,  292, 2797,  561,   35,  203,  203, 1236,   30,  203, 3667,\n",
       "         499,  773,   18, 2058,  773,   18,  203,  203,  676, 1201,   30,  203,\n",
       "         676,   16,  293,  509, 3997, 1422,  329, 3758, 1946,  292,  272, 1197,\n",
       "          18,  203,  203, 1236,   30,  203,  672,  509,  671,   16,  336,  509,\n",
       "         671,   18,  203,  203,  676, 1201,   30,  203,  949,  535, 1319,  365,\n",
       "          16,  301,  336,  460,  360, 3149,  464,  417,  834, 1086,  313,   18,\n",
       "         203,  767,  671,  263, 3319,   72, 1789,   35,  203,  203, 1236,   30,\n",
       "         203,  693,  489, 1411,  303,  373,  671,   31,  542], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:T+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_data[:T]\n",
    "y = train_data[1:T+1]\n",
    "for t in range(T):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    # print(\"when we see the text\", context, \"we predict the next character is\", target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(0, data.size(0) - T, (B,)) # 4 random locations we can sample from\n",
    "    x = torch.stack([data[i:i+T] for i in ix]) # random sequences\n",
    "    y = torch.stack([data[i+1:i+T+1] for i in ix]) # next character for each random sequence\n",
    "\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "for b in range(B):\n",
    "    for t in range(T): # for each of the characters in the sample\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    '''One Head of self-attention'''\n",
    "    def __init__(self, H):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(C, H, bias=False)\n",
    "        self.key = nn.Linear(C, H, bias=False)\n",
    "        self.value = nn.Linear(C, H, bias=False)\n",
    "        # self.output = nn.Linear(H, C, bias=False) # output matrix\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(T, T)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Query and Key matrices for the attention mechanism\n",
    "        # x: 8 tokens\n",
    "        # Q: 16 tall (arbitrary), 32 long channels\n",
    "        # K: 16 tall (arbitrary), 32 long channels\n",
    "\n",
    "        query_vectors = self.query(x)\n",
    "        key_vectors = self.key(x)\n",
    "\n",
    "\n",
    "        # Attention masking(so we can't look into the past):\n",
    "\n",
    "        tril = self.tril\n",
    "        wei = torch.zeros(T, T) \n",
    "        wei = wei.masked_fill(tril == 0, float('-inf')) # set the upper triangular to -inf\n",
    "        # xbow = wei @ x # apply the mask to the input, bag of words because simple avg.\n",
    "\n",
    "        # multiply the two to get the attention weights\n",
    "        attention_pattern = query_vectors @ key_vectors.transpose(-2, -1) # T, T\n",
    "        attention_pattern = attention_pattern / (H ** 0.5) # scale the attention pattern for numerical stability\n",
    "        attention_weights = F.softmax(attention_pattern + wei, dim=-1) # T, T (the row dimension is the query)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        value_vectors = self.value(x) # the direction we should go in the embedding space for each token (ie more blue) T, H\n",
    "\n",
    "        # apply the attention weights to the value vectors\n",
    "        context = attention_weights @ value_vectors # T, H\n",
    "\n",
    "        # project back into original space from value space\n",
    "        # return self.output(context)\n",
    "        return context\n",
    "\n",
    "x = torch.randn(B,T,C)\n",
    "head = Head(H)\n",
    "# head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    '''Multiple heads of self-attention'''\n",
    "    def __init__(self, H, C, n_heads): # H is head embedding space size, n_heads is number of heads\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(H) for _ in range(n_heads)])\n",
    "        self.combine_heads = nn.Linear(H*n_heads, C)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        x = self.combine_heads(x)  # T, C\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 128, 16])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head = MultiHeadAttention(H, C, n_heads)\n",
    "head.heads[0].forward(x).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    '''Feed-forward neural network'''\n",
    "    def __init__(self, C):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(C, C * feedforward_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(C * feedforward_factor, C),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    '''Layer normalization'''\n",
    "    def __init__(self, C, use_affine=True):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(C)) if use_affine else None\n",
    "        self.beta = nn.Parameter(torch.zeros(C)) if use_affine else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        if self.gamma is not None and self.beta is not None:\n",
    "            return self.gamma * (x - mean) / (std + 1e-6) + self.beta\n",
    "        else:\n",
    "            return (x - mean) / (std + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    '''Transformer block'''\n",
    "    def __init__(self, H, C, n_heads):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(H, C, n_heads)\n",
    "        self.ff = FeedForward(C)\n",
    "        self.norm1 = LayerNorm(C, use_affine=True)\n",
    "        self.norm2 = LayerNorm(C, use_affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.norm1(x))\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 128, 4096])\n",
      "tensor(8.7552, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.0987, -0.1158, -3.5662,  ..., -0.7042,  0.2114,  0.3453],\n",
       "          [-0.0768,  2.0443, -1.0580,  ...,  1.6488, -0.4201, -1.7179],\n",
       "          [-0.9735,  1.1225, -1.3868,  ...,  1.9589, -0.0382,  0.7307],\n",
       "          ...,\n",
       "          [ 0.8708,  1.7114, -1.0163,  ...,  0.8376, -0.1793,  0.6263],\n",
       "          [-0.4055,  1.0485, -0.7655,  ...,  1.0901,  1.0625,  0.4477],\n",
       "          [-0.0431,  2.0337, -1.9822,  ...,  0.8786,  0.1964, -1.1303]]],\n",
       "        device='cuda:0', grad_fn=<ViewBackward0>),\n",
       " None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, n_layers):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, C) \n",
    "        self.position_embedding_table = nn.Embedding(T, C)\n",
    "        self.lm_head = nn.Linear(C, vocab_size)\n",
    "        self.layers = nn.ModuleList([Block(H, C, n_heads) for _ in range(n_layers)])\n",
    "        self.block = nn.ModuleList([Block(H, C, n_heads)])\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        token_emb = self.token_embedding_table(idx) # batch_dim, sequence_dim, embedding_dim\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T))\n",
    "        x = token_emb + pos_emb # token identities and positions contained\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        logits = self.lm_head(x) # batch_dim, sequence_dim, vocab_size\n",
    "\n",
    "        batch_dim, sequence_dim, embedding_dim = logits.size()\n",
    "\n",
    "        # loss = F.cross_entropy(logits, targets) this won't work because we need 1d logits and 1d targets\n",
    "        # one-hot-vectors are a line in the x-dimension, so the shape of shape of the logits should be (-1, vocab_size).\n",
    "\n",
    "        if targets is None:\n",
    "            return logits, None\n",
    "        else:\n",
    "            # a list of all the predictions, reguardles of batch.\n",
    "            # xdim: probabilities of each character in the vocab (embedding_dim=vocab_size)\n",
    "            # ydim: all predictions for all batches flattened (batch_dim*sequence_dim)\n",
    "            logits_loss_view = logits.view(-1, vocab_size) \n",
    "            # targets loss view\n",
    "            # xdim: all targets for all batches flattened (batch_dim*sequence_dim)\n",
    "            # so this would be like, [1,4,5,1,2,3, ...]\n",
    "            # where each number is the correct next index of the one hot vector\n",
    "            targets_loss_view = targets.view(-1)\n",
    "            loss = F.cross_entropy(logits_loss_view, targets_loss_view)\n",
    "            return logits, loss\n",
    "\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx[:,-T:])\n",
    "            # get the predictions of the last token\n",
    "            last_token_logits = logits[:, -1, :] / temperature # all batches, last token, all probabilities\n",
    "            # softmax to get probabilities\n",
    "            probabilities = F.softmax(last_token_logits, dim=-1)\n",
    "            # sample from the probabilities\n",
    "            next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "            # add the new token to the idx tensor\n",
    "            idx = torch.cat((idx, next_token), dim=1)\n",
    "        return idx\n",
    "    \n",
    "\n",
    "model = GPT(n_layers)\n",
    "logits, loss = model(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_idx = torch.zeros(1, T).long()\n",
    "model.forward(idx=test_idx)\n",
    "# decode(model.generate(idx=test_idx, max_new_tokens=100)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (token_embedding_table): Embedding(4096, 256)\n",
       "  (position_embedding_table): Embedding(128, 256)\n",
       "  (lm_head): Linear(in_features=256, out_features=4096, bias=True)\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x Block(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (query): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (key): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (combine_heads): Linear(in_features=96, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.4, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=768, out_features=256, bias=True)\n",
       "          (3): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "    )\n",
       "  )\n",
       "  (block): ModuleList(\n",
       "    (0): Block(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (query): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (key): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (combine_heads): Linear(in_features=96, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.4, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=768, out_features=256, bias=True)\n",
       "          (3): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logits, loss = self(idx[:,-T:])\n",
    "\n",
    "idx = torch.zeros(1, 1).long()\n",
    "idx[:,-T:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.token_embedding_table.weight.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_iters = 10\n",
    "eval_interval = 300\n",
    "@torch.no_grad()\n",
    "def estimate_loss(is_last=False):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        real_iters = eval_iters\n",
    "        if is_last and split == 'val':  # increase last eval to mitigate noise\n",
    "            real_iters *= 10 \n",
    "        losses = torch.zeros(real_iters)\n",
    "        for k in range(real_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean() / chars_per_token\n",
    "    model.train()\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter_to_data_ratio=18.05015884495183\n",
      "token_embedding_table.weight: 1048576\n",
      "lm_head.weight: 1048576\n",
      "layers.0.ff.net.0.weight: 196608\n",
      "layers.0.ff.net.2.weight: 196608\n",
      "layers.1.ff.net.0.weight: 196608\n",
      "layers.1.ff.net.2.weight: 196608\n",
      "layers.2.ff.net.0.weight: 196608\n",
      "layers.2.ff.net.2.weight: 196608\n",
      "layers.3.ff.net.0.weight: 196608\n",
      "layers.3.ff.net.2.weight: 196608\n",
      "layers.4.ff.net.0.weight: 196608\n",
      "layers.4.ff.net.2.weight: 196608\n",
      "layers.5.ff.net.0.weight: 196608\n",
      "layers.5.ff.net.2.weight: 196608\n",
      "block.0.ff.net.0.weight: 196608\n",
      "block.0.ff.net.2.weight: 196608\n",
      "position_embedding_table.weight: 32768\n",
      "layers.0.attention.combine_heads.weight: 24576\n",
      "layers.1.attention.combine_heads.weight: 24576\n",
      "layers.2.attention.combine_heads.weight: 24576\n",
      "layers.3.attention.combine_heads.weight: 24576\n",
      "layers.4.attention.combine_heads.weight: 24576\n",
      "layers.5.attention.combine_heads.weight: 24576\n",
      "block.0.attention.combine_heads.weight: 24576\n",
      "lm_head.bias: 4096\n",
      "layers.0.attention.heads.0.query.weight: 4096\n",
      "layers.0.attention.heads.0.key.weight: 4096\n",
      "layers.0.attention.heads.0.value.weight: 4096\n",
      "layers.0.attention.heads.1.query.weight: 4096\n",
      "layers.0.attention.heads.1.key.weight: 4096\n",
      "layers.0.attention.heads.1.value.weight: 4096\n",
      "layers.0.attention.heads.2.query.weight: 4096\n",
      "layers.0.attention.heads.2.key.weight: 4096\n",
      "layers.0.attention.heads.2.value.weight: 4096\n",
      "layers.0.attention.heads.3.query.weight: 4096\n",
      "layers.0.attention.heads.3.key.weight: 4096\n",
      "layers.0.attention.heads.3.value.weight: 4096\n",
      "layers.0.attention.heads.4.query.weight: 4096\n",
      "layers.0.attention.heads.4.key.weight: 4096\n",
      "layers.0.attention.heads.4.value.weight: 4096\n",
      "layers.0.attention.heads.5.query.weight: 4096\n",
      "layers.0.attention.heads.5.key.weight: 4096\n",
      "layers.0.attention.heads.5.value.weight: 4096\n",
      "layers.1.attention.heads.0.query.weight: 4096\n",
      "layers.1.attention.heads.0.key.weight: 4096\n",
      "layers.1.attention.heads.0.value.weight: 4096\n",
      "layers.1.attention.heads.1.query.weight: 4096\n",
      "layers.1.attention.heads.1.key.weight: 4096\n",
      "layers.1.attention.heads.1.value.weight: 4096\n",
      "layers.1.attention.heads.2.query.weight: 4096\n",
      "layers.1.attention.heads.2.key.weight: 4096\n",
      "layers.1.attention.heads.2.value.weight: 4096\n",
      "layers.1.attention.heads.3.query.weight: 4096\n",
      "layers.1.attention.heads.3.key.weight: 4096\n",
      "layers.1.attention.heads.3.value.weight: 4096\n",
      "layers.1.attention.heads.4.query.weight: 4096\n",
      "layers.1.attention.heads.4.key.weight: 4096\n",
      "layers.1.attention.heads.4.value.weight: 4096\n",
      "layers.1.attention.heads.5.query.weight: 4096\n",
      "layers.1.attention.heads.5.key.weight: 4096\n",
      "layers.1.attention.heads.5.value.weight: 4096\n",
      "layers.2.attention.heads.0.query.weight: 4096\n",
      "layers.2.attention.heads.0.key.weight: 4096\n",
      "layers.2.attention.heads.0.value.weight: 4096\n",
      "layers.2.attention.heads.1.query.weight: 4096\n",
      "layers.2.attention.heads.1.key.weight: 4096\n",
      "layers.2.attention.heads.1.value.weight: 4096\n",
      "layers.2.attention.heads.2.query.weight: 4096\n",
      "layers.2.attention.heads.2.key.weight: 4096\n",
      "layers.2.attention.heads.2.value.weight: 4096\n",
      "layers.2.attention.heads.3.query.weight: 4096\n",
      "layers.2.attention.heads.3.key.weight: 4096\n",
      "layers.2.attention.heads.3.value.weight: 4096\n",
      "layers.2.attention.heads.4.query.weight: 4096\n",
      "layers.2.attention.heads.4.key.weight: 4096\n",
      "layers.2.attention.heads.4.value.weight: 4096\n",
      "layers.2.attention.heads.5.query.weight: 4096\n",
      "layers.2.attention.heads.5.key.weight: 4096\n",
      "layers.2.attention.heads.5.value.weight: 4096\n",
      "layers.3.attention.heads.0.query.weight: 4096\n",
      "layers.3.attention.heads.0.key.weight: 4096\n",
      "layers.3.attention.heads.0.value.weight: 4096\n",
      "layers.3.attention.heads.1.query.weight: 4096\n",
      "layers.3.attention.heads.1.key.weight: 4096\n",
      "layers.3.attention.heads.1.value.weight: 4096\n",
      "layers.3.attention.heads.2.query.weight: 4096\n",
      "layers.3.attention.heads.2.key.weight: 4096\n",
      "layers.3.attention.heads.2.value.weight: 4096\n",
      "layers.3.attention.heads.3.query.weight: 4096\n",
      "layers.3.attention.heads.3.key.weight: 4096\n",
      "layers.3.attention.heads.3.value.weight: 4096\n",
      "layers.3.attention.heads.4.query.weight: 4096\n",
      "layers.3.attention.heads.4.key.weight: 4096\n",
      "layers.3.attention.heads.4.value.weight: 4096\n",
      "layers.3.attention.heads.5.query.weight: 4096\n",
      "layers.3.attention.heads.5.key.weight: 4096\n",
      "layers.3.attention.heads.5.value.weight: 4096\n",
      "layers.4.attention.heads.0.query.weight: 4096\n",
      "layers.4.attention.heads.0.key.weight: 4096\n",
      "layers.4.attention.heads.0.value.weight: 4096\n",
      "layers.4.attention.heads.1.query.weight: 4096\n",
      "layers.4.attention.heads.1.key.weight: 4096\n",
      "layers.4.attention.heads.1.value.weight: 4096\n",
      "layers.4.attention.heads.2.query.weight: 4096\n",
      "layers.4.attention.heads.2.key.weight: 4096\n",
      "layers.4.attention.heads.2.value.weight: 4096\n",
      "layers.4.attention.heads.3.query.weight: 4096\n",
      "layers.4.attention.heads.3.key.weight: 4096\n",
      "layers.4.attention.heads.3.value.weight: 4096\n",
      "layers.4.attention.heads.4.query.weight: 4096\n",
      "layers.4.attention.heads.4.key.weight: 4096\n",
      "layers.4.attention.heads.4.value.weight: 4096\n",
      "layers.4.attention.heads.5.query.weight: 4096\n",
      "layers.4.attention.heads.5.key.weight: 4096\n",
      "layers.4.attention.heads.5.value.weight: 4096\n",
      "layers.5.attention.heads.0.query.weight: 4096\n",
      "layers.5.attention.heads.0.key.weight: 4096\n",
      "layers.5.attention.heads.0.value.weight: 4096\n",
      "layers.5.attention.heads.1.query.weight: 4096\n",
      "layers.5.attention.heads.1.key.weight: 4096\n",
      "layers.5.attention.heads.1.value.weight: 4096\n",
      "layers.5.attention.heads.2.query.weight: 4096\n",
      "layers.5.attention.heads.2.key.weight: 4096\n",
      "layers.5.attention.heads.2.value.weight: 4096\n",
      "layers.5.attention.heads.3.query.weight: 4096\n",
      "layers.5.attention.heads.3.key.weight: 4096\n",
      "layers.5.attention.heads.3.value.weight: 4096\n",
      "layers.5.attention.heads.4.query.weight: 4096\n",
      "layers.5.attention.heads.4.key.weight: 4096\n",
      "layers.5.attention.heads.4.value.weight: 4096\n",
      "layers.5.attention.heads.5.query.weight: 4096\n",
      "layers.5.attention.heads.5.key.weight: 4096\n",
      "layers.5.attention.heads.5.value.weight: 4096\n",
      "block.0.attention.heads.0.query.weight: 4096\n",
      "block.0.attention.heads.0.key.weight: 4096\n",
      "block.0.attention.heads.0.value.weight: 4096\n",
      "block.0.attention.heads.1.query.weight: 4096\n",
      "block.0.attention.heads.1.key.weight: 4096\n",
      "block.0.attention.heads.1.value.weight: 4096\n",
      "block.0.attention.heads.2.query.weight: 4096\n",
      "block.0.attention.heads.2.key.weight: 4096\n",
      "block.0.attention.heads.2.value.weight: 4096\n",
      "block.0.attention.heads.3.query.weight: 4096\n",
      "block.0.attention.heads.3.key.weight: 4096\n",
      "block.0.attention.heads.3.value.weight: 4096\n",
      "block.0.attention.heads.4.query.weight: 4096\n",
      "block.0.attention.heads.4.key.weight: 4096\n",
      "block.0.attention.heads.4.value.weight: 4096\n",
      "block.0.attention.heads.5.query.weight: 4096\n",
      "block.0.attention.heads.5.key.weight: 4096\n",
      "block.0.attention.heads.5.value.weight: 4096\n",
      "layers.0.ff.net.0.bias: 768\n",
      "layers.1.ff.net.0.bias: 768\n",
      "layers.2.ff.net.0.bias: 768\n",
      "layers.3.ff.net.0.bias: 768\n",
      "layers.4.ff.net.0.bias: 768\n",
      "layers.5.ff.net.0.bias: 768\n",
      "block.0.ff.net.0.bias: 768\n",
      "layers.0.attention.combine_heads.bias: 256\n",
      "layers.0.ff.net.2.bias: 256\n",
      "layers.0.norm1.gamma: 256\n",
      "layers.0.norm1.beta: 256\n",
      "layers.0.norm2.gamma: 256\n",
      "layers.0.norm2.beta: 256\n",
      "layers.1.attention.combine_heads.bias: 256\n",
      "layers.1.ff.net.2.bias: 256\n",
      "layers.1.norm1.gamma: 256\n",
      "layers.1.norm1.beta: 256\n",
      "layers.1.norm2.gamma: 256\n",
      "layers.1.norm2.beta: 256\n",
      "layers.2.attention.combine_heads.bias: 256\n",
      "layers.2.ff.net.2.bias: 256\n",
      "layers.2.norm1.gamma: 256\n",
      "layers.2.norm1.beta: 256\n",
      "layers.2.norm2.gamma: 256\n",
      "layers.2.norm2.beta: 256\n",
      "layers.3.attention.combine_heads.bias: 256\n",
      "layers.3.ff.net.2.bias: 256\n",
      "layers.3.norm1.gamma: 256\n",
      "layers.3.norm1.beta: 256\n",
      "layers.3.norm2.gamma: 256\n",
      "layers.3.norm2.beta: 256\n",
      "layers.4.attention.combine_heads.bias: 256\n",
      "layers.4.ff.net.2.bias: 256\n",
      "layers.4.norm1.gamma: 256\n",
      "layers.4.norm1.beta: 256\n",
      "layers.4.norm2.gamma: 256\n",
      "layers.4.norm2.beta: 256\n",
      "layers.5.attention.combine_heads.bias: 256\n",
      "layers.5.ff.net.2.bias: 256\n",
      "layers.5.norm1.gamma: 256\n",
      "layers.5.norm1.beta: 256\n",
      "layers.5.norm2.gamma: 256\n",
      "layers.5.norm2.beta: 256\n",
      "block.0.attention.combine_heads.bias: 256\n",
      "block.0.ff.net.2.bias: 256\n",
      "block.0.norm1.gamma: 256\n",
      "block.0.norm1.beta: 256\n",
      "block.0.norm2.gamma: 256\n",
      "block.0.norm2.beta: 256\n"
     ]
    }
   ],
   "source": [
    "# get the number of parameters\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "parameter_to_data_ratio = n_params / len(train_data)\n",
    "print(f\"{parameter_to_data_ratio=}\")\n",
    "\n",
    "parameters = []\n",
    "for name, param in model.named_parameters():\n",
    "    parameters.append({\"name\": name, \"params\": param.numel()})\n",
    "\n",
    "# sort parameters by size\n",
    "sorted_parameters = sorted(parameters, key=lambda x: x[\"params\"], reverse=True)\n",
    "for p in sorted_parameters:\n",
    "    print(f\"{p['name']}: {p['params']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [01:25<00:00, 34.99it/s]\n"
     ]
    }
   ],
   "source": [
    "num_params = sum([p.numel() for p in model.parameters()])\n",
    "\n",
    "for steps in tqdm.tqdm(range(max_iters)):\n",
    "    xb, yb = get_batch('train')\n",
    "    # loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # l2 regularization\n",
    "    l2 = sum(p.pow(2).sum() for p in model.parameters()) / num_params\n",
    "    loss = loss + l2 * l2_penalty\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        # wandb.log({\"train\": losses['train'].item(), \"val\": losses['val'].item(), \"l2\":l2})\n",
    "\n",
    "losses = estimate_loss(is_last=True)\n",
    "# wandb.log({\"train\": losses['train'].item(), \"val\": losses['val'].item()})\n",
    "# wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': tensor(0.7324, device='cuda:0'),\n",
       " 'val': tensor(1.5167, device='cuda:0')}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[38,\n",
       " 376,\n",
       " 50,\n",
       " 515,\n",
       " 51,\n",
       " 30,\n",
       " 203,\n",
       " 45,\n",
       " 389,\n",
       " 753,\n",
       " 316,\n",
       " 416,\n",
       " 548,\n",
       " 275,\n",
       " 308,\n",
       " 624,\n",
       " 5,\n",
       " 203,\n",
       " 203,\n",
       " 1321,\n",
       " 37,\n",
       " 586,\n",
       " 30,\n",
       " 203,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quote = '''BARNARDO:\n",
    "I will slay thee Horatio!\n",
    "\n",
    "HORATIO:\n",
    "'''\n",
    "# tokenize the quote\n",
    "quote_encoded = encode(quote)\n",
    "print(len(quote_encoded))\n",
    "\n",
    "# pad the quote to the length of the model\n",
    "quote_encoded = quote_encoded + [0]*(T-len(quote_encoded))\n",
    "\n",
    "quote_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(quote_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and supposed king,\n",
      "And, our king, and Clarence is deposed,\n",
      "And, by the order hath nothing,\n",
      "And nothing tempest, but green, nor England's throne\n",
      "Is prisoner to be discover'd, and that\n",
      "Adjuddy, and unsteaded friends,\n",
      "And by the seat of God's good,\n",
      "And all the rest, I'll proveth of this land.\n"
     ]
    }
   ],
   "source": [
    "test_idx = torch.zeros(1, T).long()\n",
    "print(decode(\n",
    "    model.generate(idx=test_idx, temperature=0.15, max_new_tokens=T)[0].tolist()\n",
    ")[T:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
