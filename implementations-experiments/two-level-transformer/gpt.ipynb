{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----- imports --------\n",
    "import tqdm\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "import tokenizers\n",
    "\n",
    "\n",
    "device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.set_default_device(device)\n",
    "assert device == 'cuda', \"This notebook is not optimized for CPU\"\n",
    "\n",
    "config = {\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"eval_interval\": 300,\n",
    "    \"max_iters\": 3000, \n",
    "    \"H\": 16,\n",
    "    \"B\": 64,\n",
    "    \"T\": 16,\n",
    "    \"C\": 256,\n",
    "    \"feedforward_factor\": 3,\n",
    "    \"n_heads\": 6,\n",
    "    \"dropout\": 0.4,\n",
    "    \"l2_penalty\": 0.0,\n",
    "    \"n_layers\": 6,\n",
    "    \"tokenizer_vocab_size\": 2048,\n",
    "    \"git_hash\": os.popen(\"git rev-parse HEAD\").read().strip()\n",
    "}\n",
    "\n",
    "# initial\n",
    "for k,v in config.items():\n",
    "    locals ()[k] = v\n",
    "\n",
    "\n",
    "# wandb.init(\n",
    "#     project = \"mini-shakespeare\",\n",
    "#     config = config\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in lines:  40001\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in lines: \", len(text.split('\\n')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "[262, 278, 83]\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "paths = ['input.txt']\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer()\n",
    "\n",
    "tokenizer.train(files=paths, vocab_size=tokenizer_vocab_size, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])\n",
    "\n",
    "tokenizer.save_model('.', 'shakespeare-bpe')\n",
    "\n",
    "\n",
    "\n",
    "enc = tokenizer.encode(\"Romeo Romeo wherefore art thou Romeo?\")\n",
    "tokenizer.decode(enc.ids)\n",
    "\n",
    "\n",
    "def encode(text):\n",
    "    return tokenizer.encode(text).ids\n",
    "def decode(encoded_text):\n",
    "    return tokenizer.decode(encoded_text)\n",
    "\n",
    "hello_encoded = encode(\"hello\")\n",
    "print(hello_encoded)\n",
    "print(decode(hello_encoded))\n",
    "vocab_size = tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n",
      "length of dataset in tokens:  388693\n",
      "characters per token:  2.8696014592493304\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))\n",
    "print(\"length of dataset in tokens: \", len(encode(text)))\n",
    "chars_per_token = len(text) / len(encode(text))\n",
    "print(\"characters per token: \", chars_per_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "torch.Size([388693])\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.dtype)\n",
    "print(data.size())\n",
    "print(data.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 676, 1201,   30,  203,  779,  553,  336,  589, 1817,  807, 2008,  719,\n",
       "          16,  679,  322,  621,   18], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:T+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_data[:T]\n",
    "y = train_data[1:T+1]\n",
    "for t in range(T):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    # print(\"when we see the text\", context, \"we predict the next character is\", target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch\n",
      "Input\n",
      "tensor([  17, 1415,   18,  203,   45,  360,  589,   90,  356,  320,  332,  293,\n",
      "          30,  965, 1659, 1115], device='cuda:0')\n",
      "Target\n",
      "tensor([1415,   18,  203,   45,  360,  589,   90,  356,  320,  332,  293,   30,\n",
      "         965, 1659, 1115,   16], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(0, data.size(0) - T, (B,)) # 4 random locations we can sample from\n",
    "    x = torch.stack([data[i:i+T] for i in ix]) # random sequences\n",
    "    y = torch.stack([data[i+1:i+T+1] for i in ix]) # next character for each random sequence\n",
    "\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "# for b in range(B):\n",
    "#     for t in range(T): # for each of the characters in the sample\n",
    "#         context = xb[b, :t+1]\n",
    "#         target = yb[b, t]\n",
    "\n",
    "# batch 1\n",
    "print(\"First batch\")\n",
    "print(\"Input\")\n",
    "print(xb[0])\n",
    "print(\"Target\")\n",
    "print(yb[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    '''One Head of self-attention'''\n",
    "    def __init__(self, H):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(C, H, bias=False)\n",
    "        self.key = nn.Linear(C, H, bias=False)\n",
    "        self.value = nn.Linear(C, H, bias=False)\n",
    "        # self.output = nn.Linear(H, C, bias=False) # output matrix\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(T, T)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Query and Key matrices for the attention mechanism\n",
    "        # x: 8 tokens\n",
    "        # Q: 16 tall (arbitrary), 32 long channels\n",
    "        # K: 16 tall (arbitrary), 32 long channels\n",
    "\n",
    "        query_vectors = self.query(x)\n",
    "        key_vectors = self.key(x)\n",
    "\n",
    "\n",
    "        # Attention masking(so we can't look into the past):\n",
    "\n",
    "        tril = self.tril\n",
    "        wei = torch.zeros(T, T) \n",
    "        wei = wei.masked_fill(tril == 0, float('-inf')) # set the upper triangular to -inf\n",
    "        # xbow = wei @ x # apply the mask to the input, bag of words because simple avg.\n",
    "\n",
    "        # multiply the two to get the attention weights\n",
    "        attention_pattern = query_vectors @ key_vectors.transpose(-2, -1) # T, T\n",
    "        attention_pattern = attention_pattern / (H ** 0.5) # scale the attention pattern for numerical stability\n",
    "        attention_weights = F.softmax(attention_pattern + wei, dim=-1) # T, T (the row dimension is the query)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        value_vectors = self.value(x) # the direction we should go in the embedding space for each token (ie more blue) T, H\n",
    "\n",
    "        # apply the attention weights to the value vectors\n",
    "        context = attention_weights @ value_vectors # T, H\n",
    "\n",
    "        # project back into original space from value space\n",
    "        # return self.output(context)\n",
    "        return context\n",
    "\n",
    "x = torch.randn(B,T,C)\n",
    "head = Head(H)\n",
    "# head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    '''Multiple heads of self-attention'''\n",
    "    def __init__(self, H, C, n_heads): # H is head embedding space size, n_heads is number of heads\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(H) for _ in range(n_heads)])\n",
    "        self.combine_heads = nn.Linear(H*n_heads, C)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        x = self.combine_heads(x)  # T, C\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 16, 16])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head = MultiHeadAttention(H, C, n_heads)\n",
    "head.heads[0].forward(x).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    '''Feed-forward neural network'''\n",
    "    def __init__(self, C):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(C, C * feedforward_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(C * feedforward_factor, C),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    '''Layer normalization'''\n",
    "    def __init__(self, C, use_affine=True):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(C)) if use_affine else None\n",
    "        self.beta = nn.Parameter(torch.zeros(C)) if use_affine else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        if self.gamma is not None and self.beta is not None:\n",
    "            return self.gamma * (x - mean) / (std + 1e-6) + self.beta\n",
    "        else:\n",
    "            return (x - mean) / (std + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    '''Transformer block'''\n",
    "    def __init__(self, H, C, n_heads):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(H, C, n_heads)\n",
    "        self.ff = FeedForward(C)\n",
    "        self.norm1 = LayerNorm(C, use_affine=True)\n",
    "        self.norm2 = LayerNorm(C, use_affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.norm1(x))\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 2048])\n",
      "tensor(8.0612, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-2.5261,  0.4618, -0.3474,  ...,  0.1959,  0.5735, -2.1615],\n",
       "          [-1.7736, -0.1223,  1.3924,  ...,  0.0889, -0.0904, -0.3881],\n",
       "          [-1.9511, -0.8766,  0.7249,  ...,  0.9835, -0.3902, -0.3982],\n",
       "          ...,\n",
       "          [-0.9793, -1.3599,  1.6770,  ...,  0.9981, -1.0446, -1.3555],\n",
       "          [-2.1104,  0.7004,  1.5802,  ...,  1.1575, -1.4306,  0.5271],\n",
       "          [-1.4246, -0.3286,  2.2648,  ...,  1.9164,  0.5483,  0.0211]]],\n",
       "        device='cuda:0', grad_fn=<ViewBackward0>),\n",
       " None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, n_layers):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, C) \n",
    "        self.position_embedding_table = nn.Embedding(T, C)\n",
    "        self.lm_head = nn.Linear(C, vocab_size)\n",
    "        self.layers = nn.ModuleList([Block(H, C, n_heads) for _ in range(n_layers)])\n",
    "        self.block = nn.ModuleList([Block(H, C, n_heads)])\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        token_emb = self.token_embedding_table(idx) # batch_dim, sequence_dim, embedding_dim\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T))\n",
    "        x = token_emb + pos_emb # token identities and positions contained\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        logits = self.lm_head(x) # batch_dim, sequence_dim, vocab_size\n",
    "\n",
    "        batch_dim, sequence_dim, embedding_dim = logits.size()\n",
    "\n",
    "        # loss = F.cross_entropy(logits, targets) this won't work because we need 1d logits and 1d targets\n",
    "        # one-hot-vectors are a line in the x-dimension, so the shape of shape of the logits should be (-1, vocab_size).\n",
    "\n",
    "        if targets is None:\n",
    "            return logits, None\n",
    "        else:\n",
    "            # a list of all the predictions, reguardles of batch.\n",
    "            # xdim: probabilities of each character in the vocab (embedding_dim=vocab_size)\n",
    "            # ydim: all predictions for all batches flattened (batch_dim*sequence_dim)\n",
    "            logits_loss_view = logits.view(-1, vocab_size) \n",
    "            # targets loss view\n",
    "            # xdim: all targets for all batches flattened (batch_dim*sequence_dim)\n",
    "            # so this would be like, [1,4,5,1,2,3, ...]\n",
    "            # where each number is the correct next index of the one hot vector\n",
    "            targets_loss_view = targets.view(-1)\n",
    "            loss = F.cross_entropy(logits_loss_view, targets_loss_view)\n",
    "            return logits, loss\n",
    "\n",
    "\n",
    "    def generate_next_token(self, idx, temperature=1.0):\n",
    "        # ensure there is only one batch \n",
    "        assert idx.size(0) == 1\n",
    "        # padd the idx tensor to the right with zeros to make it T long\n",
    "        if idx.size(1) < T:\n",
    "            padded_idx = F.pad(idx, (0, T - idx.size(1)), value=0)\n",
    "        else:\n",
    "            padded_idx = idx\n",
    "        # logits, loss = self(idx[:,-T:]) # get the logits for the last T tokens\n",
    "        logits, loss = self(padded_idx[:,-T:]) # get the logits for the last T tokens\n",
    "        # get the predictions of the last token\n",
    "        last_token_logits = logits[:, -1, :] / temperature # all batches, last token, all probabilities\n",
    "        all_token_logits = logits\n",
    "\n",
    "        # softmax to get probabilities for all tokens\n",
    "\n",
    "\n",
    "\n",
    "        # softmax to get probabilities\n",
    "        probabilities = F.softmax(last_token_logits, dim=-1)\n",
    "        # sample from the probabilities\n",
    "        next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "    \n",
    "\n",
    "model = GPT(n_layers)\n",
    "logits, loss = model(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_idx = torch.zeros(1, T).long()\n",
    "model.forward(idx=test_idx)\n",
    "# decode(model.generate(idx=test_idx, max_new_tokens=100)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = torch.tensor([[1,2,3]])\n",
    "prediction_index = idx.size(1)-1\n",
    "idx[:, prediction_index]\n",
    "# pad the idx tensor to the right with zeros to make it T long\n",
    "# padded_idx = F.pad(idx, (0, T - idx.size(1)), value=0)\n",
    "# print(padded_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logits, loss = self(idx[:,-T:])\n",
    "\n",
    "idx = torch.zeros(1, 1).long()\n",
    "idx[:,-T:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.token_embedding_table.weight.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_iters = 10\n",
    "eval_interval = 300\n",
    "@torch.no_grad()\n",
    "def estimate_loss(is_last=False):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        real_iters = eval_iters\n",
    "        if is_last and split == 'val':  # increase last eval to mitigate noise\n",
    "            real_iters *= 10 \n",
    "        losses = torch.zeros(real_iters)\n",
    "        for k in range(real_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean() / chars_per_token\n",
    "    model.train()\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter_to_data_ratio=12.896487652327034\n",
      "token_embedding_table.weight: 524288\n",
      "lm_head.weight: 524288\n",
      "layers.0.ff.net.0.weight: 196608\n",
      "layers.0.ff.net.2.weight: 196608\n",
      "layers.1.ff.net.0.weight: 196608\n",
      "layers.1.ff.net.2.weight: 196608\n",
      "layers.2.ff.net.0.weight: 196608\n",
      "layers.2.ff.net.2.weight: 196608\n",
      "layers.3.ff.net.0.weight: 196608\n",
      "layers.3.ff.net.2.weight: 196608\n",
      "layers.4.ff.net.0.weight: 196608\n",
      "layers.4.ff.net.2.weight: 196608\n",
      "layers.5.ff.net.0.weight: 196608\n",
      "layers.5.ff.net.2.weight: 196608\n",
      "block.0.ff.net.0.weight: 196608\n",
      "block.0.ff.net.2.weight: 196608\n",
      "layers.0.attention.combine_heads.weight: 24576\n",
      "layers.1.attention.combine_heads.weight: 24576\n",
      "layers.2.attention.combine_heads.weight: 24576\n",
      "layers.3.attention.combine_heads.weight: 24576\n",
      "layers.4.attention.combine_heads.weight: 24576\n",
      "layers.5.attention.combine_heads.weight: 24576\n",
      "block.0.attention.combine_heads.weight: 24576\n",
      "position_embedding_table.weight: 4096\n",
      "layers.0.attention.heads.0.query.weight: 4096\n",
      "layers.0.attention.heads.0.key.weight: 4096\n",
      "layers.0.attention.heads.0.value.weight: 4096\n",
      "layers.0.attention.heads.1.query.weight: 4096\n",
      "layers.0.attention.heads.1.key.weight: 4096\n",
      "layers.0.attention.heads.1.value.weight: 4096\n",
      "layers.0.attention.heads.2.query.weight: 4096\n",
      "layers.0.attention.heads.2.key.weight: 4096\n",
      "layers.0.attention.heads.2.value.weight: 4096\n",
      "layers.0.attention.heads.3.query.weight: 4096\n",
      "layers.0.attention.heads.3.key.weight: 4096\n",
      "layers.0.attention.heads.3.value.weight: 4096\n",
      "layers.0.attention.heads.4.query.weight: 4096\n",
      "layers.0.attention.heads.4.key.weight: 4096\n",
      "layers.0.attention.heads.4.value.weight: 4096\n",
      "layers.0.attention.heads.5.query.weight: 4096\n",
      "layers.0.attention.heads.5.key.weight: 4096\n",
      "layers.0.attention.heads.5.value.weight: 4096\n",
      "layers.1.attention.heads.0.query.weight: 4096\n",
      "layers.1.attention.heads.0.key.weight: 4096\n",
      "layers.1.attention.heads.0.value.weight: 4096\n",
      "layers.1.attention.heads.1.query.weight: 4096\n",
      "layers.1.attention.heads.1.key.weight: 4096\n",
      "layers.1.attention.heads.1.value.weight: 4096\n",
      "layers.1.attention.heads.2.query.weight: 4096\n",
      "layers.1.attention.heads.2.key.weight: 4096\n",
      "layers.1.attention.heads.2.value.weight: 4096\n",
      "layers.1.attention.heads.3.query.weight: 4096\n",
      "layers.1.attention.heads.3.key.weight: 4096\n",
      "layers.1.attention.heads.3.value.weight: 4096\n",
      "layers.1.attention.heads.4.query.weight: 4096\n",
      "layers.1.attention.heads.4.key.weight: 4096\n",
      "layers.1.attention.heads.4.value.weight: 4096\n",
      "layers.1.attention.heads.5.query.weight: 4096\n",
      "layers.1.attention.heads.5.key.weight: 4096\n",
      "layers.1.attention.heads.5.value.weight: 4096\n",
      "layers.2.attention.heads.0.query.weight: 4096\n",
      "layers.2.attention.heads.0.key.weight: 4096\n",
      "layers.2.attention.heads.0.value.weight: 4096\n",
      "layers.2.attention.heads.1.query.weight: 4096\n",
      "layers.2.attention.heads.1.key.weight: 4096\n",
      "layers.2.attention.heads.1.value.weight: 4096\n",
      "layers.2.attention.heads.2.query.weight: 4096\n",
      "layers.2.attention.heads.2.key.weight: 4096\n",
      "layers.2.attention.heads.2.value.weight: 4096\n",
      "layers.2.attention.heads.3.query.weight: 4096\n",
      "layers.2.attention.heads.3.key.weight: 4096\n",
      "layers.2.attention.heads.3.value.weight: 4096\n",
      "layers.2.attention.heads.4.query.weight: 4096\n",
      "layers.2.attention.heads.4.key.weight: 4096\n",
      "layers.2.attention.heads.4.value.weight: 4096\n",
      "layers.2.attention.heads.5.query.weight: 4096\n",
      "layers.2.attention.heads.5.key.weight: 4096\n",
      "layers.2.attention.heads.5.value.weight: 4096\n",
      "layers.3.attention.heads.0.query.weight: 4096\n",
      "layers.3.attention.heads.0.key.weight: 4096\n",
      "layers.3.attention.heads.0.value.weight: 4096\n",
      "layers.3.attention.heads.1.query.weight: 4096\n",
      "layers.3.attention.heads.1.key.weight: 4096\n",
      "layers.3.attention.heads.1.value.weight: 4096\n",
      "layers.3.attention.heads.2.query.weight: 4096\n",
      "layers.3.attention.heads.2.key.weight: 4096\n",
      "layers.3.attention.heads.2.value.weight: 4096\n",
      "layers.3.attention.heads.3.query.weight: 4096\n",
      "layers.3.attention.heads.3.key.weight: 4096\n",
      "layers.3.attention.heads.3.value.weight: 4096\n",
      "layers.3.attention.heads.4.query.weight: 4096\n",
      "layers.3.attention.heads.4.key.weight: 4096\n",
      "layers.3.attention.heads.4.value.weight: 4096\n",
      "layers.3.attention.heads.5.query.weight: 4096\n",
      "layers.3.attention.heads.5.key.weight: 4096\n",
      "layers.3.attention.heads.5.value.weight: 4096\n",
      "layers.4.attention.heads.0.query.weight: 4096\n",
      "layers.4.attention.heads.0.key.weight: 4096\n",
      "layers.4.attention.heads.0.value.weight: 4096\n",
      "layers.4.attention.heads.1.query.weight: 4096\n",
      "layers.4.attention.heads.1.key.weight: 4096\n",
      "layers.4.attention.heads.1.value.weight: 4096\n",
      "layers.4.attention.heads.2.query.weight: 4096\n",
      "layers.4.attention.heads.2.key.weight: 4096\n",
      "layers.4.attention.heads.2.value.weight: 4096\n",
      "layers.4.attention.heads.3.query.weight: 4096\n",
      "layers.4.attention.heads.3.key.weight: 4096\n",
      "layers.4.attention.heads.3.value.weight: 4096\n",
      "layers.4.attention.heads.4.query.weight: 4096\n",
      "layers.4.attention.heads.4.key.weight: 4096\n",
      "layers.4.attention.heads.4.value.weight: 4096\n",
      "layers.4.attention.heads.5.query.weight: 4096\n",
      "layers.4.attention.heads.5.key.weight: 4096\n",
      "layers.4.attention.heads.5.value.weight: 4096\n",
      "layers.5.attention.heads.0.query.weight: 4096\n",
      "layers.5.attention.heads.0.key.weight: 4096\n",
      "layers.5.attention.heads.0.value.weight: 4096\n",
      "layers.5.attention.heads.1.query.weight: 4096\n",
      "layers.5.attention.heads.1.key.weight: 4096\n",
      "layers.5.attention.heads.1.value.weight: 4096\n",
      "layers.5.attention.heads.2.query.weight: 4096\n",
      "layers.5.attention.heads.2.key.weight: 4096\n",
      "layers.5.attention.heads.2.value.weight: 4096\n",
      "layers.5.attention.heads.3.query.weight: 4096\n",
      "layers.5.attention.heads.3.key.weight: 4096\n",
      "layers.5.attention.heads.3.value.weight: 4096\n",
      "layers.5.attention.heads.4.query.weight: 4096\n",
      "layers.5.attention.heads.4.key.weight: 4096\n",
      "layers.5.attention.heads.4.value.weight: 4096\n",
      "layers.5.attention.heads.5.query.weight: 4096\n",
      "layers.5.attention.heads.5.key.weight: 4096\n",
      "layers.5.attention.heads.5.value.weight: 4096\n",
      "block.0.attention.heads.0.query.weight: 4096\n",
      "block.0.attention.heads.0.key.weight: 4096\n",
      "block.0.attention.heads.0.value.weight: 4096\n",
      "block.0.attention.heads.1.query.weight: 4096\n",
      "block.0.attention.heads.1.key.weight: 4096\n",
      "block.0.attention.heads.1.value.weight: 4096\n",
      "block.0.attention.heads.2.query.weight: 4096\n",
      "block.0.attention.heads.2.key.weight: 4096\n",
      "block.0.attention.heads.2.value.weight: 4096\n",
      "block.0.attention.heads.3.query.weight: 4096\n",
      "block.0.attention.heads.3.key.weight: 4096\n",
      "block.0.attention.heads.3.value.weight: 4096\n",
      "block.0.attention.heads.4.query.weight: 4096\n",
      "block.0.attention.heads.4.key.weight: 4096\n",
      "block.0.attention.heads.4.value.weight: 4096\n",
      "block.0.attention.heads.5.query.weight: 4096\n",
      "block.0.attention.heads.5.key.weight: 4096\n",
      "block.0.attention.heads.5.value.weight: 4096\n",
      "lm_head.bias: 2048\n",
      "layers.0.ff.net.0.bias: 768\n",
      "layers.1.ff.net.0.bias: 768\n",
      "layers.2.ff.net.0.bias: 768\n",
      "layers.3.ff.net.0.bias: 768\n",
      "layers.4.ff.net.0.bias: 768\n",
      "layers.5.ff.net.0.bias: 768\n",
      "block.0.ff.net.0.bias: 768\n",
      "layers.0.attention.combine_heads.bias: 256\n",
      "layers.0.ff.net.2.bias: 256\n",
      "layers.0.norm1.gamma: 256\n",
      "layers.0.norm1.beta: 256\n",
      "layers.0.norm2.gamma: 256\n",
      "layers.0.norm2.beta: 256\n",
      "layers.1.attention.combine_heads.bias: 256\n",
      "layers.1.ff.net.2.bias: 256\n",
      "layers.1.norm1.gamma: 256\n",
      "layers.1.norm1.beta: 256\n",
      "layers.1.norm2.gamma: 256\n",
      "layers.1.norm2.beta: 256\n",
      "layers.2.attention.combine_heads.bias: 256\n",
      "layers.2.ff.net.2.bias: 256\n",
      "layers.2.norm1.gamma: 256\n",
      "layers.2.norm1.beta: 256\n",
      "layers.2.norm2.gamma: 256\n",
      "layers.2.norm2.beta: 256\n",
      "layers.3.attention.combine_heads.bias: 256\n",
      "layers.3.ff.net.2.bias: 256\n",
      "layers.3.norm1.gamma: 256\n",
      "layers.3.norm1.beta: 256\n",
      "layers.3.norm2.gamma: 256\n",
      "layers.3.norm2.beta: 256\n",
      "layers.4.attention.combine_heads.bias: 256\n",
      "layers.4.ff.net.2.bias: 256\n",
      "layers.4.norm1.gamma: 256\n",
      "layers.4.norm1.beta: 256\n",
      "layers.4.norm2.gamma: 256\n",
      "layers.4.norm2.beta: 256\n",
      "layers.5.attention.combine_heads.bias: 256\n",
      "layers.5.ff.net.2.bias: 256\n",
      "layers.5.norm1.gamma: 256\n",
      "layers.5.norm1.beta: 256\n",
      "layers.5.norm2.gamma: 256\n",
      "layers.5.norm2.beta: 256\n",
      "block.0.attention.combine_heads.bias: 256\n",
      "block.0.ff.net.2.bias: 256\n",
      "block.0.norm1.gamma: 256\n",
      "block.0.norm1.beta: 256\n",
      "block.0.norm2.gamma: 256\n",
      "block.0.norm2.beta: 256\n"
     ]
    }
   ],
   "source": [
    "# get the number of parameters\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "parameter_to_data_ratio = n_params / len(train_data)\n",
    "print(f\"{parameter_to_data_ratio=}\")\n",
    "\n",
    "parameters = []\n",
    "for name, param in model.named_parameters():\n",
    "    parameters.append({\"name\": name, \"params\": param.numel()})\n",
    "\n",
    "# sort parameters by size\n",
    "sorted_parameters = sorted(parameters, key=lambda x: x[\"params\"], reverse=True)\n",
    "for p in sorted_parameters:\n",
    "    print(f\"{p['name']}: {p['params']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [01:09<00:00, 43.07it/s]\n"
     ]
    }
   ],
   "source": [
    "num_params = sum([p.numel() for p in model.parameters()])\n",
    "\n",
    "for steps in tqdm.tqdm(range(max_iters)):\n",
    "    xb, yb = get_batch('train')\n",
    "    # loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # l2 regularization\n",
    "    l2 = sum(p.pow(2).sum() for p in model.parameters()) / num_params\n",
    "    loss = loss + l2 * l2_penalty\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        # wandb.log({\"train\": losses['train'].item(), \"val\": losses['val'].item(), \"l2\":l2})\n",
    "\n",
    "losses = estimate_loss(is_last=True)\n",
    "# wandb.log({\"train\": losses['train'].item(), \"val\": losses['val'].item()})\n",
    "# wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': tensor(1.2042, device='cuda:0'),\n",
       " 'val': tensor(1.4577, device='cuda:0')}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_idx = torch.zeros(1, T).long()\n",
    "\n",
    "# quote = '''KING '''\n",
    "# # I will slay thee Horatio!\n",
    "\n",
    "# # HORATIO:\n",
    "\n",
    "# # tokenize the quote\n",
    "# quote_encoded = encode(quote)\n",
    "# print(\"length of input\", len(quote_encoded))\n",
    "\n",
    "# quote_encoded = torch.tensor([quote_encoded], dtype=torch.long)\n",
    "# print(quote_encoded)\n",
    "\n",
    "\n",
    "# print(decode(\n",
    "#     model.generate(idx=quote_encoded, temperature=.1, max_new_tokens=T)[0].tolist()\n",
    "# )[T:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction index 1\n",
      "quote encoded tensor([[863,  30]], device='cuda:0')\n",
      "quote padded tensor([[863,  30, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203,\n",
      "         203, 203]], device='cuda:0')\n",
      "next token prediction: '\n",
      "'\n"
     ]
    }
   ],
   "source": [
    "# Predicting with right padding\n",
    "\n",
    "newline_encoded = encode('\\n')[0]\n",
    "quote = '''ROMEO:'''\n",
    "quote_encoded = torch.tensor([encode(quote)])\n",
    "prediction_index = quote_encoded.size(1) - 1\n",
    "print(\"prediction index\", prediction_index)\n",
    "print(\"quote encoded\", quote_encoded)\n",
    "\n",
    "# pad the idx tensor to the right with zeros to make it T long\n",
    "quote_padded = F.pad(quote_encoded, (0, T - quote_encoded.size(1)), value=newline_encoded)\n",
    "print(\"quote padded\", quote_padded)\n",
    "logits, loss = model(quote_padded)\n",
    "\n",
    "prediction = logits[:, prediction_index, :]\n",
    "probs = F.softmax(prediction, dim=-1)\n",
    "next_token = torch.argmax(probs, dim=-1)\n",
    "print(f\"next token prediction: '{decode([next_token.item()])}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quote padded tensor([[203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203,\n",
      "         863,  30]], device='cuda:0')\n",
      "left-pad next token prediction: '\n",
      "'\n",
      "====================\n",
      "quote padded tensor([[203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203,\n",
      "         863,  30]], device='cuda:0')\n",
      "left-pad next token prediction: '\n",
      "'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[203]], device='cuda:0')"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_next_token_pad_right(quote_encoded):\n",
    "    newline_encoded = encode('\\n')[0]\n",
    "    prediction_index = quote_encoded.size(1) - 1\n",
    "    print(\"prediction index\", prediction_index)\n",
    "    print(\"quote encoded\", quote_encoded)\n",
    "\n",
    "    # pad the idx tensor to the right with zeros to make it T long\n",
    "    quote_padded = F.pad(quote_encoded, (0, T - quote_encoded.size(1)), value=newline_encoded)\n",
    "    print(\"quote padded\", quote_padded)\n",
    "    logits, loss = model(quote_padded)\n",
    "\n",
    "    prediction = logits[:, prediction_index, :]\n",
    "    probs = F.softmax(prediction, dim=-1)\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "    print(f\"right-pad next token prediction: '{decode([next_token.item()])}'\")\n",
    "    return next_token\n",
    "\n",
    "\n",
    "def predict_token_pad_left(quote_encoded):\n",
    "    newline_encoded = encode('\\n')[0]\n",
    "    # pad on the left\n",
    "    quote_padded = F.pad(quote_encoded, (T - quote_encoded.size(1), 0), value=newline_encoded)\n",
    "    print(\"quote padded\", quote_padded)\n",
    "    logits, loss = model(quote_padded)\n",
    "    prediction = logits[:, T - 1, :]\n",
    "    probs = F.softmax(prediction, dim=-1)\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "    print(f\"left-pad next token prediction: '{decode([next_token.item()])}'\")\n",
    "    return next_token\n",
    "\n",
    "quote = \"ROMEO:\"\n",
    "tokens = torch.tensor([encode(quote)])\n",
    "\n",
    "predict_token_pad_left(tokens)\n",
    "print(\"=\"*20)\n",
    "predict_token_pad_left(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quote padded tensor([[203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203,\n",
      "         863,  30]], device='cuda:0')\n",
      "next token prediction: '\n",
      "'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([203], device='cuda:0')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
