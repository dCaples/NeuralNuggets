{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load dataset\n",
    "\n",
    "# from datasets import load_dataset\n",
    "# from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "# tokenizer = ByteLevelBPETokenizer()\n",
    "# dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "\n",
    "# # Specify the split you want to save (e.g., \"train\", \"validation\", \"test\")\n",
    "# split = \"train\"\n",
    "\n",
    "# # Get the desired split from the dataset\n",
    "# subset = dataset[split]\n",
    "\n",
    "# # Save the subset to a text file\n",
    "# subset.to_csv(\"tinystories-train.txt\", sep=\"\\t\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----- imports --------\n",
    "\n",
    "import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import wandb\n",
    "import os\n",
    "import tokenizers\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.set_default_device(device)\n",
    "assert device == 'cuda', \"This notebook is not optimized for CPU\"\n",
    "\n",
    "config = {\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"sae_learning_rate\": 5e-5,\n",
    "    \"model_embedding_layer\": 6,\n",
    "    \"eval_interval\": 500,\n",
    "    \"max_iters\": 60000, \n",
    "    \"H\": 32, # hidden dimension size\n",
    "    \"B\": 64,\n",
    "    \"T\": 256,\n",
    "    \"C\": 256,\n",
    "    \"feedforward_factor\": 3,\n",
    "    \"n_heads\": 8,\n",
    "    \"n_layers\": 12,\n",
    "    \"tokenizer_vocab_size\": 2**13,\n",
    "    \"git_hash\": os.popen(\"git rev-parse HEAD\").read().strip()\n",
    "}\n",
    "\n",
    "# initial\n",
    "for k,v in config.items():\n",
    "    locals ()[k] = v\n",
    "\n",
    "\n",
    "#wandb.init(\n",
    "#    project = \"tinystories\",\n",
    "#    config = config,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# stories_data = []\n",
    "# data_dir = './data'\n",
    "# for filename in os.listdir(data_dir):\n",
    "#     file_path = os.path.join(data_dir, filename)\n",
    "#     if filename.endswith('.json'):\n",
    "#         with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#             data = json.load(f)\n",
    "#             stories_data.extend(data)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the tinystories tokenizer\n",
    "# tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "#     \"./tiny-stories-bpe-vocab.json\", \n",
    "#     \"./tiny-stories-bpe-merges.txt\"\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# def encode(text):\n",
    "#     return torch.tensor(tokenizer.encode(text).ids, dtype=torch.int64)\n",
    "# def decode(encoded_text):\n",
    "#     return tokenizer.decode(encoded_text.tolist())\n",
    "\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# encoded_stories = [encode(story['story']) for story in tqdm(stories_data, desc=\"Encoding stories\")]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the encoded stories to a file\n",
    "# torch.save(encoded_stories, 'encoded-stories.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('tinystories-train.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1916206969\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "479051742.25"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1916206969/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in lines:  20550005\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in lines: \", len(text.split('\\n')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text\n",
      "\"One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
      "\n",
      "Lily went to her mom and said, \"\"Mom, I found this needle. Can you share it with me and sew my shirt?\"\" Her mom smiled and said, \"\"Yes, Lily, we can share the needle and fix your shirt.\"\"\n",
      "\n",
      "Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.\"\n",
      "\"Once upon a time, there was a little car named Beep. Beep loved to go fast and play in the sun. Beep was a healthy car because he always had good fuel. Good fuel made Beep happy and strong.\n",
      "\n",
      "One day, Beep was driving in the park when he saw a big tree. The tree had many leaves that wer\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths = ['tinystories-train.txt']\n",
    "# tokenizer = tokenizers.ByteLevelBPETokenizer()\n",
    "\n",
    "# tokenizer.train(files=paths, vocab_size=tokenizer_vocab_size, min_frequency=2)\n",
    "\n",
    "# tokenizer.save_model('.', 'tiny-stories-bpe')\n",
    "\n",
    "\n",
    "\n",
    "# enc = tokenizer.encode(\"She sells sea shells by the sea shore!\")\n",
    "# tokenizer.decode(enc.ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    \"./tiny-stories-bpe-vocab.json\", \n",
    "    \"./tiny-stories-bpe-merges.txt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6132]\n",
      "hello\n",
      "vocab size:  8192\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def encode(text):\n",
    "    return tokenizer.encode(text).ids\n",
    "def decode(encoded_text):\n",
    "    return tokenizer.decode(encoded_text)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def batch_encode(text, batch_size):\n",
    "    tokens = []\n",
    "    for i in tqdm(range(0, len(text), batch_size)):\n",
    "        tokens.extend(encode(text[i:i+batch_size]))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "hello_encoded = encode(\"hello\")\n",
    "print(hello_encoded)\n",
    "print(decode(hello_encoded))\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(\"vocab size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 61.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory used by sample_encoded:  1.2918853759765625 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sample_text = text[:200000]\n",
    "sample_encoded = batch_encode(sample_text, 20000)\n",
    "\n",
    "# get the amount of memory used by sample_encoded\n",
    "def recursive_memory_usage(python_obj):\n",
    "    if isinstance(python_obj, (str, int, float)):\n",
    "        return python_obj.__sizeof__()\n",
    "    if isinstance(python_obj, dict):\n",
    "        return sum([recursive_memory_usage(v) for v in python_obj.values()])\n",
    "    if isinstance(python_obj, list):\n",
    "        return sum([recursive_memory_usage(v) for v in python_obj])\n",
    "    return python_obj.__sizeof__()\n",
    "\n",
    "print(\"memory used by sample_encoded: \", recursive_memory_usage(sample_encoded) / 1024**2, \"MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  10000\n",
      "length of dataset in tokens:  2457\n",
      "characters per token:  4.07000407000407\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text[:10000]))\n",
    "print(\"length of dataset in tokens: \", len(encode(text[:10000])))\n",
    "chars_per_token = len(text[:10000]) / len(encode(text[:10000]))\n",
    "print(\"characters per token: \", chars_per_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_text = batch_encode(text, 200000)\n",
    "# # data = torch.tensor(encode(text), dtype=torch.int64)\n",
    "# data = torch.tensor(encoded_text, dtype=torch.int64, device='cuda')\n",
    "# print(data.dtype)\n",
    "# print(data.size())\n",
    "# print(data.device)\n",
    "# torch.save(data, 'tiny-stories-train.pt')\n",
    "# encoded_text = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from tiny-stories-train.pt\n",
    "data = torch.load('tiny-stories-train.pt', map_location='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "468832276"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([421949048])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  83, 3206,  198,    1,  421,  356,   11,  258,  397,  447,  501,  364,\n",
       "         596,  258, 3736,  316,  309,  759,   13,  313,  704,  304,  282, 2966,\n",
       "         265,  359,  342,  304,  788,  304,  282, 2120,   13,  364,  445,  265,\n",
       "         949,  262, 3736,  342,  309,  365,   11,  350,  338,  461, 5198,  258,\n",
       "        2228,  345,  309, 2500,   13,  198,  198,  343,  469,  265,  309,  365,\n",
       "         264,  327,   11,  329,  771,   11,  335,  596,  741, 3736,   13, 1282,\n",
       "         346,  949,  304,  342,  519,  264, 5198,  652, 2500,  478,  866,  365,\n",
       "         499,  264,  327,   11,  329,  832,   11,  364,   11,  363,  472,  949,\n",
       "         262, 3736,  264, 1306,  627, 2500,  416,  198,  198, 4625,   11,  362,\n",
       "        1656,  262, 3736,  264, 7930,  262, 2228,  345,  364,  371, 2500,   13,\n",
       "         410,  282,  385, 2966,  366,  449,  788,  362,  430, 2502,  264, 1762,\n",
       "         757,  573,   13, 1453,  362, 1444,   11,  364,  858,  309,  365,  366,\n",
       "        2502,  262, 3736,  264, 5150,  309, 2500,   13,  320,  897,  514,  405,\n",
       "         788,  362,  360, 1656,  264, 1370,  567,  408,  198,    1,  432,  448,\n",
       "         258,  396,   11,  400,  282,  258,  397,  565,  501, 2632,  592,   13,\n",
       "        2632,  592,  504,  265,  437,  848,  264,  359,  316,  262,  734,   13,\n",
       "        2632,  592,  282,  258, 2188,  565,  788,  278,  667,  360,  590, 4830,\n",
       "          13, 5204, 4830,  564, 2632,  592,  405,  264, 1123,   13,  198,  198,\n",
       "         421,  356,   11, 2632,  592,  282, 3476,  316,  262,  570,  589,  278,\n",
       "         414,  258,  407,  680,   13,  299,  680,  360,  791, 1508,  357,  430,\n",
       "        4175,   13, 2632,  592,  616,  713,  262, 1508, 1544,  264,  445,  265,\n",
       "         359,  342,  449,   13, 2632], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:T+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text\\n\"One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"\"Mom, I found this needle. Can you share it with me and sew my shirt?\"\" Her mom smiled and said, \"\"Yes, Lily, we can share the needle and fix your shirt.\"\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.\"\\n\"Once upon a time, there was a little car named Beep. Beep loved to go fast and play in the sun. Beep was a healthy car because he always had good fuel. Good fuel made Beep happy and strong.\\n\\nOne day, Beep was driving in the park when he saw a big tree. The tree had many leaves that were falling. Beep liked how the leaves fall and wanted to play with them. Be'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(train_data[:T+1].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_data[:T]\n",
    "y = train_data[1:T+1]\n",
    "for t in range(T):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    # print(\"when we see the text\", context, \"we predict the next character is\", target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "# torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    '''One Head of self-attention'''\n",
    "    def __init__(self, H):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(C, H, bias=False)\n",
    "        self.key = nn.Linear(C, H, bias=False)\n",
    "        self.value = nn.Linear(C, H, bias=False)\n",
    "        # self.output = nn.Linear(H, C, bias=False) # output matrix\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(T, T)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Query and Key matrices for the attention mechanism\n",
    "        # x: 8 tokens\n",
    "        # Q: 16 tall (arbitrary), 32 long channels\n",
    "        # K: 16 tall (arbitrary), 32 long channels\n",
    "\n",
    "        query_vectors = self.query(x)\n",
    "        key_vectors = self.key(x)\n",
    "\n",
    "\n",
    "        # Attention masking(so we can't look into the past):\n",
    "\n",
    "        tril = self.tril\n",
    "        wei = torch.zeros(T, T) \n",
    "        wei = wei.masked_fill(tril == 0, float('-inf')) # set the upper triangular to -inf\n",
    "        # xbow = wei @ x # apply the mask to the input, bag of words because simple avg.\n",
    "\n",
    "        # multiply the two to get the attention weights\n",
    "        attention_pattern = query_vectors @ key_vectors.transpose(-2, -1) # T, T\n",
    "        attention_pattern = attention_pattern / (H ** 0.5) # scale the attention pattern for numerical stability\n",
    "        attention_weights = F.softmax(attention_pattern + wei, dim=-1) # T, T (the row dimension is the query)\n",
    "\n",
    "        value_vectors = self.value(x) # the direction we should go in the embedding space for each token (ie more blue) T, H\n",
    "\n",
    "        # apply the attention weights to the value vectors\n",
    "        context = attention_weights @ value_vectors # T, H\n",
    "\n",
    "        # project back into original space from value space\n",
    "        # return self.output(context)\n",
    "        return context\n",
    "\n",
    "x = torch.randn(B,T,C)\n",
    "head = Head(H)\n",
    "# head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    '''Multiple heads of self-attention'''\n",
    "    def __init__(self, H, C, n_heads): # H is head embedding space size, n_heads is number of heads\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(H) for _ in range(n_heads)])\n",
    "        self.combine_heads = nn.Linear(H*n_heads, C)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        x = self.combine_heads(x)  # T, C\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 32])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head = MultiHeadAttention(H, C, n_heads)\n",
    "head.heads[0].forward(x).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    '''Feed-forward neural network'''\n",
    "    def __init__(self, C):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(C, C * feedforward_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(C * feedforward_factor, C),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    '''Layer normalization'''\n",
    "    def __init__(self, C, use_affine=True):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(C)) if use_affine else None\n",
    "        self.beta = nn.Parameter(torch.zeros(C)) if use_affine else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        if self.gamma is not None and self.beta is not None:\n",
    "            return self.gamma * (x - mean) / (std + 1e-6) + self.beta\n",
    "        else:\n",
    "            return (x - mean) / (std + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    '''Transformer block'''\n",
    "    def __init__(self, H, C, n_heads):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(H, C, n_heads)\n",
    "        self.ff = FeedForward(C)\n",
    "        self.norm1 = LayerNorm(C, use_affine=True)\n",
    "        self.norm2 = LayerNorm(C, use_affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.norm1(x))\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, n_layers):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, C) \n",
    "        self.position_embedding_table = nn.Embedding(T, C)\n",
    "        self.lm_head = nn.Linear(C, vocab_size)\n",
    "        self.layers = nn.ModuleList([Block(H, C, n_heads) for _ in range(n_layers)])\n",
    "    \n",
    "    def forward(self, idx, targets=None, return_residuals=None):\n",
    "        B, T = idx.shape\n",
    "        token_emb = self.token_embedding_table(idx) # batch_dim, sequence_dim, embedding_dim\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T))\n",
    "        x = token_emb + pos_emb # token identities and positions contained\n",
    "\n",
    "        if return_residuals == \"first_embedding\":\n",
    "            return x\n",
    "\n",
    "        def excess_kurtosis(emb):\n",
    "            mean = torch.mean(emb, dim=-1, keepdim=True) # BxTx1\n",
    "            std = torch.std(emb, dim=-1, keepdim=True) # BxTx1\n",
    "\n",
    "            centralized = emb - mean #BxTxC\n",
    "            fourth_moment = torch.mean(centralized**4, dim=-1, keepdim=True) # BxTx1\n",
    "            kurtosis = torch.squeeze(fourth_moment / std**4, dim=-1) # BxT\n",
    "            # view as a 1d vector\n",
    "            kurtosis = kurtosis.view(-1) - 3\n",
    "            # make each one min 0\n",
    "            kurtosis = torch.maximum(kurtosis, torch.tensor(0.0))\n",
    "            # sum over the vector\n",
    "            kurtosis = torch.sum(kurtosis)\n",
    "            return kurtosis\n",
    "\n",
    "\n",
    "        kurtosis_sum = torch.tensor(0.0)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            kurtosis_sum += excess_kurtosis(x)\n",
    "            if return_residuals is not None and i == return_residuals:\n",
    "                return x\n",
    "        \n",
    "        kurtosis_avg = kurtosis_sum / (len(self.layers) * T * B)\n",
    "\n",
    "        logits = self.lm_head(x) # batch_dim, sequence_dim, vocab_size\n",
    "\n",
    "        batch_dim, sequence_dim, embedding_dim = logits.size()\n",
    "\n",
    "        # loss = F.cross_entropy(logits, targets) this won't work because we need 1d logits and 1d targets\n",
    "        # one-hot-vectors are a line in the x-dimension, so the shape of shape of the logits should be (-1, vocab_size).\n",
    "\n",
    "        if targets is None:\n",
    "            return logits, None, kurtosis_avg\n",
    "        else:\n",
    "            # a list of all the predictions, reguardles of batch.\n",
    "            # xdim: probabilities of each character in the vocab (embedding_dim=vocab_size)\n",
    "            # ydim: all predictions for all batches flattened (batch_dim*sequence_dim)\n",
    "            logits_loss_view = logits.view(-1, vocab_size) \n",
    "            # targets loss view\n",
    "            # xdim: all targets for all batches flattened (batch_dim*sequence_dim)\n",
    "            # so this would be like, [1,4,5,1,2,3, ...]\n",
    "            # where each number is the correct next index of the one hot vector\n",
    "            targets_loss_view = targets.view(-1)\n",
    "            loss = F.cross_entropy(logits_loss_view, targets_loss_view)\n",
    "            return logits, loss, kurtosis_avg\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, temperature=0.5):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx[:,-T:])\n",
    "            # get the predictions of the last token\n",
    "            last_token_logits = logits[:, -1, :] # all batches, last token, all probabilities\n",
    "            # apply temperature\n",
    "            last_token_logits = last_token_logits / temperature\n",
    "            # softmax to get probabilities\n",
    "            probabilities = F.softmax(last_token_logits, dim=-1)\n",
    "            # sample from the probabilities\n",
    "            next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "            # add the new token to the idx tensor\n",
    "            idx = torch.cat((idx, next_token), dim=1)\n",
    "        return idx\n",
    "    def prompt_model(self, prompt, max_new_tokens, temperature=0.5):\n",
    "        autoregressive_seq = encode(prompt)\n",
    "        for _ in range(max_new_tokens):\n",
    "            prediction_index = len(autoregressive_seq)-1\n",
    "\n",
    "            model_input = torch.tensor(autoregressive_seq)\n",
    "            \n",
    "            while model_input.shape[0] < T:\n",
    "                pad_token = torch.tensor(encode(\"\\n\"))\n",
    "                model_input = torch.cat((model_input, pad_token), dim=0)\n",
    "\n",
    "            model_input\n",
    "            model_input = model_input.unsqueeze(0)\n",
    "\n",
    "            logits, loss, kurtosis_avg = model(model_input)\n",
    "            prediction_token = logits[:, prediction_index, :] / temperature\n",
    "            probabilities = F.softmax(prediction_token, dim=-1)\n",
    "            next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "            next_token = next_token.item()\n",
    "\n",
    "            autoregressive_seq.append(next_token)\n",
    "        # get the autoregressive sequence\n",
    "        return decode(autoregressive_seq)\n",
    "    def get_embedding(self, prompt, override_model_embedding_layer=None):\n",
    "        if override_model_embedding_layer is None:\n",
    "            selected_model_embedding_layer = model_embedding_layer\n",
    "        else:\n",
    "            selected_model_embedding_layer = override_model_embedding_layer\n",
    "        sequence = encode(prompt)\n",
    "        model_input = torch.tensor(sequence)\n",
    "        sequence_index = len(sequence) - 1\n",
    "        while model_input.shape[0] < T:\n",
    "            pad_token = torch.tensor(encode(\"\\n\"))\n",
    "            model_input = torch.cat((model_input, pad_token), dim=0)\n",
    "        model_input = model_input.unsqueeze(0)\n",
    "        embedding = self.forward(model_input, return_residuals=selected_model_embedding_layer)\n",
    "        # remove the batch dimension\n",
    "        embedding = embedding.squeeze(0)[sequence_index]\n",
    "        return embedding\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "model = GPT(n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model\n",
    "model.load_state_dict(torch.load('tiny-stories-model-v2-kurtosis-penalty.pt'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# saving embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x71e46fec4230>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Freeze model parameters and disable building compute backprop graph\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25753/25753 [07:24<00:00, 57.94it/s]\n",
      "100%|██████████| 2861/2861 [00:50<00:00, 56.66it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_context_window(split, ix):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    x = torch.stack([data[i:i+T] for i in ix]) # random sequences\n",
    "    y = torch.stack([data[i+1:i+T+1] for i in ix]) # next character for each random sequence\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "accumulated_residuals = []\n",
    "residuals_per_save = 2_000\n",
    "save_idx = 0\n",
    "ratio_residuals_save = 0.1\n",
    "\n",
    "with torch.no_grad():\n",
    "    for split in ['train', 'val']:\n",
    "        data = train_data if split == 'train' else val_data\n",
    "        tokens_in_batch = B*T\n",
    "        for i in tqdm(range(0, len(data)-tokens_in_batch, tokens_in_batch)): # the - tokens_in_batch is there so we skip the last, potentially unfull batch\n",
    "            # B ixs, step is T, start at i\n",
    "            ixs = torch.arange(i, i+tokens_in_batch, T)\n",
    "            xb, yb = get_context_window(split, ixs)\n",
    "            residuals = model(xb, return_residuals=model_embedding_layer)\n",
    "            residuals_flattened = residuals.view(-1, T)\n",
    "            indices = torch.randperm(tokens_in_batch)[:int(tokens_in_batch*ratio_residuals_save)]\n",
    "            sampled_residuals = residuals_flattened[indices].clone()\n",
    "            del residuals\n",
    "            del residuals_flattened\n",
    "\n",
    "            accumulated_residuals.append(sampled_residuals)\n",
    "            if len(accumulated_residuals) >= residuals_per_save:\n",
    "                torch.save(accumulated_residuals, f\"residuals/residuals_{split}_{save_idx}.pt\")\n",
    "                accumulated_residuals = []\n",
    "                save_idx += 1\n",
    "        torch.save(accumulated_residuals, f\"residuals/residuals_{split}_{save_idx}.pt\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
