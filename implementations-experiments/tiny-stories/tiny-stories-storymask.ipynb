{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----- imports --------\n",
    "import tqdm\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "import tokenizers\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.set_default_device(device)\n",
    "assert device == 'cuda', \"This notebook is not optimized for CPU\"\n",
    "\n",
    "config = {\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"eval_interval\": 300,\n",
    "    \"max_iters\": 60000, \n",
    "    \"H\": 32, # hidden dimension size\n",
    "    \"B\": 64,\n",
    "    \"T\": 256,\n",
    "    \"C\": 256,\n",
    "    \"feedforward_factor\": 3,\n",
    "    \"n_heads\": 8,\n",
    "    \"dropout\": 0.0,\n",
    "    \"l2_penalty\": 0.0,\n",
    "    \"n_layers\": 12,\n",
    "    \"tokenizer_vocab_size\": 2**13,\n",
    "    \"git_hash\": os.popen(\"git rev-parse HEAD\").read().strip()\n",
    "}\n",
    "\n",
    "# initial\n",
    "for k,v in config.items():\n",
    "    locals ()[k] = v\n",
    "\n",
    "\n",
    "#wandb.init(\n",
    "#    project = \"tinystories\",\n",
    "#    config = config,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('TinyStoriesV2-GPT4-valid.txt', 'r', encoding='utf-8') as f:\n",
    "    stories = f.read().split('<|endoftext|>')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOnce upon a time, in a hot and sandy land, there was a little camel named Cally. Cally lived near an oasis, a place with water and trees. She loved to play there with her friends.\\nOne day, a fierce lion came to the oasis. He was very big and scary. The lion roared and demanded, \"Give me all your food!\" Cally and her friends were scared, but they wanted to save their oasis.\\nCally had an idea. She told her friends to make a big, loud noise. They stomped their feet and yelled together. The lion was scared of the noise and ran away. Cally and her friends saved the oasis and were very happy.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stories[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'She sells sea shells by the sea shore!'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# paths = ['tinystories-train.txt']\n",
    "# tokenizer = tokenizers.ByteLevelBPETokenizer()\n",
    "\n",
    "# tokenizer.train(files=paths, vocab_size=tokenizer_vocab_size, min_frequency=2)\n",
    "\n",
    "# tokenizer.save_model('.', 'tiny-stories-bpe')\n",
    "\n",
    "\n",
    "\n",
    "# enc = tokenizer.encode(\"She sells sea shells by the sea shore!\")\n",
    "# tokenizer.decode(enc.ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    \"./tiny-stories-bpe-vocab.json\", \n",
    "    \"./tiny-stories-bpe-merges.txt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6132], device='cuda:0')\n",
      "hello\n",
      "vocab size:  8192\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def encode(text):\n",
    "    return torch.tensor(tokenizer.encode(text).ids, dtype=torch.int64)\n",
    "def decode(encoded_text):\n",
    "    return tokenizer.decode(encoded_text.tolist())\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "hello_encoded = encode(\"hello\")\n",
    "print(hello_encoded)\n",
    "print(decode(hello_encoded))\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(\"vocab size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories_encoded = [encode(story) for story in stories]\n",
    "\n",
    "# save stores_encoded to a file\n",
    "torch.save(stories_encoded, 'stories_encoded.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bin(remaining_capacity=-1, items=[6])\n",
      "Bin(remaining_capacity=0, items=[4, 1])\n",
      "Bin(remaining_capacity=3, items=[2])\n"
     ]
    }
   ],
   "source": [
    "class Bin:\n",
    "    def __init__(self, bin_capacity):\n",
    "        self.remaining_capacity = bin_capacity\n",
    "        self.items = []\n",
    "    def add_item(self, item):\n",
    "        self.items.append(item)\n",
    "        self.remaining_capacity -= item\n",
    "    def fits(self, item):\n",
    "        return self.remaining_capacity >= item\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Bin(remaining_capacity={self.remaining_capacity}, items={self.items})'\n",
    "\n",
    "def put_in_bin(bins, bin_capacity, item):\n",
    "    for bin in bins:\n",
    "        if bin.fits(item):\n",
    "            bin.add_item(item)\n",
    "            break\n",
    "    else:\n",
    "        newbin = Bin(bin_capacity)\n",
    "        newbin.add_item(item)\n",
    "        bins.append(newbin)\n",
    "\n",
    "\n",
    "\n",
    "def first_fit_sequence_packing(list_of_lengths, max_length):\n",
    "    list_of_lengths.sort(reverse=True)\n",
    "    bins = []\n",
    "    for length in list_of_lengths:\n",
    "        put_in_bin(bins, max_length, length)\n",
    "    return bins\n",
    "\n",
    "\n",
    "\n",
    "bins = first_fit_sequence_packing([6, 2, 1, 4], 5)\n",
    "for bin in bins:\n",
    "    print(bin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of bins 11296\n"
     ]
    }
   ],
   "source": [
    "encoded_story_lengths = [len(story) for story in stories_encoded]\n",
    "\n",
    "context_length = 512\n",
    "bins = first_fit_sequence_packing(encoded_story_lengths, context_length)\n",
    "print(\"number of bins\", len(bins))\n",
    "\n",
    "total_remaining_capacity = 0\n",
    "for bin in bins:\n",
    "    if bin.remaining_capacity > 0:\n",
    "        total_remaining_capacity += bin.remaining_capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.569670679886684"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_remaining_capacity/len(bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the length of each encoded story\n",
    "encoded_story_lengths = [len(story) for story in stories_encoded]\n",
    "\n",
    "# Plot the histogram\n",
    "plt.hist(encoded_story_lengths, bins=50, edgecolor='black')\n",
    "plt.title('Histogram of Encoded Story Lengths')\n",
    "\n",
    "plt.xlabel('Length of Encoded Story')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the length of each story\n",
    "story_lengths = [len(story) for story in stories]\n",
    "\n",
    "# Sort the story lengths\n",
    "sorted_lengths = np.sort(story_lengths)\n",
    "\n",
    "# Calculate the cumulative distribution\n",
    "cdf = np.arange(1, len(sorted_lengths) + 1) / len(sorted_lengths)\n",
    "\n",
    "# Plot the CDF\n",
    "plt.plot(sorted_lengths, cdf, marker='.', linestyle='none')\n",
    "plt.title('Cumulative Distribution Function of Story Lengths')\n",
    "plt.xlabel('Length of Story')\n",
    "plt.ylabel('CDF')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total characters: 81562, total tokens: 20461\n",
      "characters per token: 3.986217682420214\n"
     ]
    }
   ],
   "source": [
    "total_chars = 0\n",
    "total_tokens = 0\n",
    "for story in stories[:100]:\n",
    "    total_chars += len(story)\n",
    "    total_tokens += len(encode(story))\n",
    "\n",
    "print(f\"total characters: {total_chars}, total tokens: {total_tokens}\")\n",
    "print(f\"characters per token: {total_chars / total_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyStoriesDataset(Dataset):\n",
    "    def __init__(self, stories_encoded):\n",
    "        self.stories_encoded = stories_encoded\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.stories_encoded)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.stories_encoded[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u don\\'t have to be scared of the loud dog, I\\'ll protect you\". The mole felt so safe with the little girl. She was very kind and the mole soon came to trust her. He leaned against her and she kept him safe. The mole had found his best friend.\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tinystoriesdataset = TinyStoriesDataset(stories_encoded)\n",
    "decode(tinystoriesdataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tinystoriesdataloader = DataLoader(tinystoriesdataset, batch_size=B, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from tiny-stories-train.pt\n",
    "data = torch.load('tiny-stories-train.pt', map_location='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([421949048])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  83, 3206,  198,    1,  421,  356,   11,  258,  397,  447,  501,  364,\n",
       "         596,  258, 3736,  316,  309,  759,   13,  313,  704,  304,  282, 2966,\n",
       "         265,  359,  342,  304,  788,  304,  282, 2120,   13,  364,  445,  265,\n",
       "         949,  262, 3736,  342,  309,  365,   11,  350,  338,  461, 5198,  258,\n",
       "        2228,  345,  309, 2500,   13,  198,  198,  343,  469,  265,  309,  365,\n",
       "         264,  327,   11,  329,  771,   11,  335,  596,  741, 3736,   13, 1282,\n",
       "         346,  949,  304,  342,  519,  264, 5198,  652, 2500,  478,  866,  365,\n",
       "         499,  264,  327,   11,  329,  832,   11,  364,   11,  363,  472,  949,\n",
       "         262, 3736,  264, 1306,  627, 2500,  416,  198,  198, 4625,   11,  362,\n",
       "        1656,  262, 3736,  264, 7930,  262, 2228,  345,  364,  371, 2500,   13,\n",
       "         410,  282,  385, 2966,  366,  449,  788,  362,  430, 2502,  264, 1762,\n",
       "         757,  573,   13, 1453,  362, 1444,   11,  364,  858,  309,  365,  366,\n",
       "        2502,  262, 3736,  264, 5150,  309, 2500,   13,  320,  897,  514,  405,\n",
       "         788,  362,  360, 1656,  264, 1370,  567,  408,  198,    1,  432,  448,\n",
       "         258,  396,   11,  400,  282,  258,  397,  565,  501, 2632,  592,   13,\n",
       "        2632,  592,  504,  265,  437,  848,  264,  359,  316,  262,  734,   13,\n",
       "        2632,  592,  282,  258, 2188,  565,  788,  278,  667,  360,  590, 4830,\n",
       "          13, 5204, 4830,  564, 2632,  592,  405,  264, 1123,   13,  198,  198,\n",
       "         421,  356,   11, 2632,  592,  282, 3476,  316,  262,  570,  589,  278,\n",
       "         414,  258,  407,  680,   13,  299,  680,  360,  791, 1508,  357,  430,\n",
       "        4175,   13, 2632,  592,  616,  713,  262, 1508, 1544,  264,  445,  265,\n",
       "         359,  342,  449,   13, 2632], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:T+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text\\n\"One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"\"Mom, I found this needle. Can you share it with me and sew my shirt?\"\" Her mom smiled and said, \"\"Yes, Lily, we can share the needle and fix your shirt.\"\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.\"\\n\"Once upon a time, there was a little car named Beep. Beep loved to go fast and play in the sun. Beep was a healthy car because he always had good fuel. Good fuel made Beep happy and strong.\\n\\nOne day, Beep was driving in the park when he saw a big tree. The tree had many leaves that were falling. Beep liked how the leaves fall and wanted to play with them. Be'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(train_data[:T+1].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_data[:T]\n",
    "y = train_data[1:T+1]\n",
    "for t in range(T):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    # print(\"when we see the text\", context, \"we predict the next character is\", target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(0, data.size(0) - T, (B,)) # 4 random locations we can sample from\n",
    "    x = torch.stack([data[i:i+T] for i in ix]) # random sequences\n",
    "    y = torch.stack([data[i+1:i+T+1] for i in ix]) # next character for each random sequence\n",
    "\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "for b in range(B):\n",
    "    for t in range(T): # for each of the characters in the sample\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    '''One Head of self-attention'''\n",
    "    def __init__(self, H):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(C, H, bias=False)\n",
    "        self.key = nn.Linear(C, H, bias=False)\n",
    "        self.value = nn.Linear(C, H, bias=False)\n",
    "        # self.output = nn.Linear(H, C, bias=False) # output matrix\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(T, T)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Query and Key matrices for the attention mechanism\n",
    "        # x: 8 tokens\n",
    "        # Q: 16 tall (arbitrary), 32 long channels\n",
    "        # K: 16 tall (arbitrary), 32 long channels\n",
    "\n",
    "        query_vectors = self.query(x)\n",
    "        key_vectors = self.key(x)\n",
    "\n",
    "\n",
    "        # Attention masking(so we can't look into the past):\n",
    "\n",
    "        tril = self.tril\n",
    "        wei = torch.zeros(T, T) \n",
    "        wei = wei.masked_fill(tril == 0, float('-inf')) # set the upper triangular to -inf\n",
    "        # xbow = wei @ x # apply the mask to the input, bag of words because simple avg.\n",
    "\n",
    "        # multiply the two to get the attention weights\n",
    "        attention_pattern = query_vectors @ key_vectors.transpose(-2, -1) # T, T\n",
    "        attention_pattern = attention_pattern / (H ** 0.5) # scale the attention pattern for numerical stability\n",
    "        attention_weights = F.softmax(attention_pattern + wei, dim=-1) # T, T (the row dimension is the query)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        value_vectors = self.value(x) # the direction we should go in the embedding space for each token (ie more blue) T, H\n",
    "\n",
    "        # apply the attention weights to the value vectors\n",
    "        context = attention_weights @ value_vectors # T, H\n",
    "\n",
    "        # project back into original space from value space\n",
    "        # return self.output(context)\n",
    "        return context\n",
    "\n",
    "x = torch.randn(B,T,C)\n",
    "head = Head(H)\n",
    "# head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    '''Multiple heads of self-attention'''\n",
    "    def __init__(self, H, C, n_heads): # H is head embedding space size, n_heads is number of heads\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(H) for _ in range(n_heads)])\n",
    "        self.combine_heads = nn.Linear(H*n_heads, C)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        x = self.combine_heads(x)  # T, C\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 32])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head = MultiHeadAttention(H, C, n_heads)\n",
    "head.heads[0].forward(x).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    '''Feed-forward neural network'''\n",
    "    def __init__(self, C):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(C, C * feedforward_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(C * feedforward_factor, C),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    '''Layer normalization'''\n",
    "    def __init__(self, C, use_affine=True):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(C)) if use_affine else None\n",
    "        self.beta = nn.Parameter(torch.zeros(C)) if use_affine else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        if self.gamma is not None and self.beta is not None:\n",
    "            return self.gamma * (x - mean) / (std + 1e-6) + self.beta\n",
    "        else:\n",
    "            return (x - mean) / (std + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    '''Transformer block'''\n",
    "    def __init__(self, H, C, n_heads):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(H, C, n_heads)\n",
    "        self.ff = FeedForward(C)\n",
    "        self.norm1 = LayerNorm(C, use_affine=True)\n",
    "        self.norm2 = LayerNorm(C, use_affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.norm1(x))\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 83\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m decode(autoregressive_seq)\n\u001b[1;32m     82\u001b[0m model \u001b[38;5;241m=\u001b[39m GPT(n_layers)\n\u001b[0;32m---> 83\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m model(\u001b[43mxb\u001b[49m, yb)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(logits\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xb' is not defined"
     ]
    }
   ],
   "source": [
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, n_layers):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, C) \n",
    "        self.position_embedding_table = nn.Embedding(T, C)\n",
    "        self.lm_head = nn.Linear(C, vocab_size)\n",
    "        self.layers = nn.ModuleList([Block(H, C, n_heads) for _ in range(n_layers)])\n",
    "        self.block = nn.ModuleList([Block(H, C, n_heads)])\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        token_emb = self.token_embedding_table(idx) # batch_dim, sequence_dim, embedding_dim\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T))\n",
    "        x = token_emb + pos_emb # token identities and positions contained\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        logits = self.lm_head(x) # batch_dim, sequence_dim, vocab_size\n",
    "\n",
    "        batch_dim, sequence_dim, embedding_dim = logits.size()\n",
    "\n",
    "        # loss = F.cross_entropy(logits, targets) this won't work because we need 1d logits and 1d targets\n",
    "        # one-hot-vectors are a line in the x-dimension, so the shape of shape of the logits should be (-1, vocab_size).\n",
    "\n",
    "        if targets is None:\n",
    "            return logits, None\n",
    "        else:\n",
    "            # a list of all the predictions, reguardles of batch.\n",
    "            # xdim: probabilities of each character in the vocab (embedding_dim=vocab_size)\n",
    "            # ydim: all predictions for all batches flattened (batch_dim*sequence_dim)\n",
    "            logits_loss_view = logits.view(-1, vocab_size) \n",
    "            # targets loss view\n",
    "            # xdim: all targets for all batches flattened (batch_dim*sequence_dim)\n",
    "            # so this would be like, [1,4,5,1,2,3, ...]\n",
    "            # where each number is the correct next index of the one hot vector\n",
    "            targets_loss_view = targets.view(-1)\n",
    "            loss = F.cross_entropy(logits_loss_view, targets_loss_view)\n",
    "            return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx[:,-T:])\n",
    "            # get the predictions of the last token\n",
    "            last_token_logits = logits[:, -1, :] # all batches, last token, all probabilities\n",
    "            # softmax to get probabilities\n",
    "            probabilities = F.softmax(last_token_logits, dim=-1)\n",
    "            # sample from the probabilities\n",
    "            next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "            # add the new token to the idx tensor\n",
    "            idx = torch.cat((idx, next_token), dim=1)\n",
    "        return idx\n",
    "    def prompt_model(self, prompt, max_new_tokens, temperature=0.5):\n",
    "        autoregressive_seq = encode(prompt)\n",
    "        for _ in range(max_new_tokens):\n",
    "            prediction_index = len(autoregressive_seq)-1\n",
    "\n",
    "            model_input = torch.tensor(autoregressive_seq)\n",
    "            \n",
    "            while model_input.shape[0] < T:\n",
    "                pad_token = torch.tensor(encode(\"\\n\"))\n",
    "                model_input = torch.cat((model_input, pad_token), dim=0)\n",
    "\n",
    "            model_input\n",
    "            model_input = model_input.unsqueeze(0)\n",
    "\n",
    "            logits, loss = model(model_input)\n",
    "            prediction_token = logits[:, prediction_index, :] / temperature\n",
    "            probabilities = F.softmax(prediction_token, dim=-1)\n",
    "            next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "            next_token = next_token.item()\n",
    "\n",
    "            autoregressive_seq.append(next_token)\n",
    "        # get the autoregressive sequence\n",
    "        return decode(autoregressive_seq)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "model = GPT(n_layers)\n",
    "logits, loss = model(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_idx = torch.zeros(1, T).long()\n",
    "model.forward(idx=test_idx)\n",
    "# decode(model.generate(idx=test_idx, max_new_tokens=100)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (token_embedding_table): Embedding(8192, 256)\n",
       "  (position_embedding_table): Embedding(256, 256)\n",
       "  (lm_head): Linear(in_features=256, out_features=8192, bias=True)\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x Block(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (query): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (key): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (combine_heads): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=768, out_features=256, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "    )\n",
       "  )\n",
       "  (block): ModuleList(\n",
       "    (0): Block(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (query): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (key): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (combine_heads): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=768, out_features=256, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters in the model:  12817664\n"
     ]
    }
   ],
   "source": [
    "# get the number of parameters in the model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"number of parameters in the model: \", count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([468832276])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logits, loss = self(idx[:,-T:])\n",
    "\n",
    "idx = torch.zeros(1, 1).long()\n",
    "idx[:,-T:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.token_embedding_table.weight.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_iters = 10\n",
    "eval_interval = 300\n",
    "@torch.no_grad()\n",
    "def estimate_loss(is_last=False):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        real_iters = eval_iters\n",
    "        if is_last and split == 'val':  # increase last eval to mitigate noise\n",
    "            real_iters *= 10 \n",
    "        losses = torch.zeros(real_iters)\n",
    "        for k in range(real_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean() / chars_per_token\n",
    "    model.train()\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter_to_data_ratio=0.03037727910693141\n",
      "token_embedding_table.weight: 2097152\n",
      "lm_head.weight: 2097152\n",
      "layers.0.ff.net.0.weight: 196608\n",
      "layers.0.ff.net.2.weight: 196608\n",
      "layers.1.ff.net.0.weight: 196608\n",
      "layers.1.ff.net.2.weight: 196608\n",
      "layers.2.ff.net.0.weight: 196608\n",
      "layers.2.ff.net.2.weight: 196608\n",
      "layers.3.ff.net.0.weight: 196608\n",
      "layers.3.ff.net.2.weight: 196608\n",
      "layers.4.ff.net.0.weight: 196608\n",
      "layers.4.ff.net.2.weight: 196608\n",
      "layers.5.ff.net.0.weight: 196608\n",
      "layers.5.ff.net.2.weight: 196608\n",
      "layers.6.ff.net.0.weight: 196608\n",
      "layers.6.ff.net.2.weight: 196608\n",
      "layers.7.ff.net.0.weight: 196608\n",
      "layers.7.ff.net.2.weight: 196608\n",
      "layers.8.ff.net.0.weight: 196608\n",
      "layers.8.ff.net.2.weight: 196608\n",
      "layers.9.ff.net.0.weight: 196608\n",
      "layers.9.ff.net.2.weight: 196608\n",
      "layers.10.ff.net.0.weight: 196608\n",
      "layers.10.ff.net.2.weight: 196608\n",
      "layers.11.ff.net.0.weight: 196608\n",
      "layers.11.ff.net.2.weight: 196608\n",
      "block.0.ff.net.0.weight: 196608\n",
      "block.0.ff.net.2.weight: 196608\n",
      "position_embedding_table.weight: 65536\n",
      "layers.0.attention.combine_heads.weight: 65536\n",
      "layers.1.attention.combine_heads.weight: 65536\n",
      "layers.2.attention.combine_heads.weight: 65536\n",
      "layers.3.attention.combine_heads.weight: 65536\n",
      "layers.4.attention.combine_heads.weight: 65536\n",
      "layers.5.attention.combine_heads.weight: 65536\n",
      "layers.6.attention.combine_heads.weight: 65536\n",
      "layers.7.attention.combine_heads.weight: 65536\n",
      "layers.8.attention.combine_heads.weight: 65536\n",
      "layers.9.attention.combine_heads.weight: 65536\n",
      "layers.10.attention.combine_heads.weight: 65536\n",
      "layers.11.attention.combine_heads.weight: 65536\n",
      "block.0.attention.combine_heads.weight: 65536\n",
      "lm_head.bias: 8192\n",
      "layers.0.attention.heads.0.query.weight: 8192\n",
      "layers.0.attention.heads.0.key.weight: 8192\n",
      "layers.0.attention.heads.0.value.weight: 8192\n",
      "layers.0.attention.heads.1.query.weight: 8192\n",
      "layers.0.attention.heads.1.key.weight: 8192\n",
      "layers.0.attention.heads.1.value.weight: 8192\n",
      "layers.0.attention.heads.2.query.weight: 8192\n",
      "layers.0.attention.heads.2.key.weight: 8192\n",
      "layers.0.attention.heads.2.value.weight: 8192\n",
      "layers.0.attention.heads.3.query.weight: 8192\n",
      "layers.0.attention.heads.3.key.weight: 8192\n",
      "layers.0.attention.heads.3.value.weight: 8192\n",
      "layers.0.attention.heads.4.query.weight: 8192\n",
      "layers.0.attention.heads.4.key.weight: 8192\n",
      "layers.0.attention.heads.4.value.weight: 8192\n",
      "layers.0.attention.heads.5.query.weight: 8192\n",
      "layers.0.attention.heads.5.key.weight: 8192\n",
      "layers.0.attention.heads.5.value.weight: 8192\n",
      "layers.0.attention.heads.6.query.weight: 8192\n",
      "layers.0.attention.heads.6.key.weight: 8192\n",
      "layers.0.attention.heads.6.value.weight: 8192\n",
      "layers.0.attention.heads.7.query.weight: 8192\n",
      "layers.0.attention.heads.7.key.weight: 8192\n",
      "layers.0.attention.heads.7.value.weight: 8192\n",
      "layers.1.attention.heads.0.query.weight: 8192\n",
      "layers.1.attention.heads.0.key.weight: 8192\n",
      "layers.1.attention.heads.0.value.weight: 8192\n",
      "layers.1.attention.heads.1.query.weight: 8192\n",
      "layers.1.attention.heads.1.key.weight: 8192\n",
      "layers.1.attention.heads.1.value.weight: 8192\n",
      "layers.1.attention.heads.2.query.weight: 8192\n",
      "layers.1.attention.heads.2.key.weight: 8192\n",
      "layers.1.attention.heads.2.value.weight: 8192\n",
      "layers.1.attention.heads.3.query.weight: 8192\n",
      "layers.1.attention.heads.3.key.weight: 8192\n",
      "layers.1.attention.heads.3.value.weight: 8192\n",
      "layers.1.attention.heads.4.query.weight: 8192\n",
      "layers.1.attention.heads.4.key.weight: 8192\n",
      "layers.1.attention.heads.4.value.weight: 8192\n",
      "layers.1.attention.heads.5.query.weight: 8192\n",
      "layers.1.attention.heads.5.key.weight: 8192\n",
      "layers.1.attention.heads.5.value.weight: 8192\n",
      "layers.1.attention.heads.6.query.weight: 8192\n",
      "layers.1.attention.heads.6.key.weight: 8192\n",
      "layers.1.attention.heads.6.value.weight: 8192\n",
      "layers.1.attention.heads.7.query.weight: 8192\n",
      "layers.1.attention.heads.7.key.weight: 8192\n",
      "layers.1.attention.heads.7.value.weight: 8192\n",
      "layers.2.attention.heads.0.query.weight: 8192\n",
      "layers.2.attention.heads.0.key.weight: 8192\n",
      "layers.2.attention.heads.0.value.weight: 8192\n",
      "layers.2.attention.heads.1.query.weight: 8192\n",
      "layers.2.attention.heads.1.key.weight: 8192\n",
      "layers.2.attention.heads.1.value.weight: 8192\n",
      "layers.2.attention.heads.2.query.weight: 8192\n",
      "layers.2.attention.heads.2.key.weight: 8192\n",
      "layers.2.attention.heads.2.value.weight: 8192\n",
      "layers.2.attention.heads.3.query.weight: 8192\n",
      "layers.2.attention.heads.3.key.weight: 8192\n",
      "layers.2.attention.heads.3.value.weight: 8192\n",
      "layers.2.attention.heads.4.query.weight: 8192\n",
      "layers.2.attention.heads.4.key.weight: 8192\n",
      "layers.2.attention.heads.4.value.weight: 8192\n",
      "layers.2.attention.heads.5.query.weight: 8192\n",
      "layers.2.attention.heads.5.key.weight: 8192\n",
      "layers.2.attention.heads.5.value.weight: 8192\n",
      "layers.2.attention.heads.6.query.weight: 8192\n",
      "layers.2.attention.heads.6.key.weight: 8192\n",
      "layers.2.attention.heads.6.value.weight: 8192\n",
      "layers.2.attention.heads.7.query.weight: 8192\n",
      "layers.2.attention.heads.7.key.weight: 8192\n",
      "layers.2.attention.heads.7.value.weight: 8192\n",
      "layers.3.attention.heads.0.query.weight: 8192\n",
      "layers.3.attention.heads.0.key.weight: 8192\n",
      "layers.3.attention.heads.0.value.weight: 8192\n",
      "layers.3.attention.heads.1.query.weight: 8192\n",
      "layers.3.attention.heads.1.key.weight: 8192\n",
      "layers.3.attention.heads.1.value.weight: 8192\n",
      "layers.3.attention.heads.2.query.weight: 8192\n",
      "layers.3.attention.heads.2.key.weight: 8192\n",
      "layers.3.attention.heads.2.value.weight: 8192\n",
      "layers.3.attention.heads.3.query.weight: 8192\n",
      "layers.3.attention.heads.3.key.weight: 8192\n",
      "layers.3.attention.heads.3.value.weight: 8192\n",
      "layers.3.attention.heads.4.query.weight: 8192\n",
      "layers.3.attention.heads.4.key.weight: 8192\n",
      "layers.3.attention.heads.4.value.weight: 8192\n",
      "layers.3.attention.heads.5.query.weight: 8192\n",
      "layers.3.attention.heads.5.key.weight: 8192\n",
      "layers.3.attention.heads.5.value.weight: 8192\n",
      "layers.3.attention.heads.6.query.weight: 8192\n",
      "layers.3.attention.heads.6.key.weight: 8192\n",
      "layers.3.attention.heads.6.value.weight: 8192\n",
      "layers.3.attention.heads.7.query.weight: 8192\n",
      "layers.3.attention.heads.7.key.weight: 8192\n",
      "layers.3.attention.heads.7.value.weight: 8192\n",
      "layers.4.attention.heads.0.query.weight: 8192\n",
      "layers.4.attention.heads.0.key.weight: 8192\n",
      "layers.4.attention.heads.0.value.weight: 8192\n",
      "layers.4.attention.heads.1.query.weight: 8192\n",
      "layers.4.attention.heads.1.key.weight: 8192\n",
      "layers.4.attention.heads.1.value.weight: 8192\n",
      "layers.4.attention.heads.2.query.weight: 8192\n",
      "layers.4.attention.heads.2.key.weight: 8192\n",
      "layers.4.attention.heads.2.value.weight: 8192\n",
      "layers.4.attention.heads.3.query.weight: 8192\n",
      "layers.4.attention.heads.3.key.weight: 8192\n",
      "layers.4.attention.heads.3.value.weight: 8192\n",
      "layers.4.attention.heads.4.query.weight: 8192\n",
      "layers.4.attention.heads.4.key.weight: 8192\n",
      "layers.4.attention.heads.4.value.weight: 8192\n",
      "layers.4.attention.heads.5.query.weight: 8192\n",
      "layers.4.attention.heads.5.key.weight: 8192\n",
      "layers.4.attention.heads.5.value.weight: 8192\n",
      "layers.4.attention.heads.6.query.weight: 8192\n",
      "layers.4.attention.heads.6.key.weight: 8192\n",
      "layers.4.attention.heads.6.value.weight: 8192\n",
      "layers.4.attention.heads.7.query.weight: 8192\n",
      "layers.4.attention.heads.7.key.weight: 8192\n",
      "layers.4.attention.heads.7.value.weight: 8192\n",
      "layers.5.attention.heads.0.query.weight: 8192\n",
      "layers.5.attention.heads.0.key.weight: 8192\n",
      "layers.5.attention.heads.0.value.weight: 8192\n",
      "layers.5.attention.heads.1.query.weight: 8192\n",
      "layers.5.attention.heads.1.key.weight: 8192\n",
      "layers.5.attention.heads.1.value.weight: 8192\n",
      "layers.5.attention.heads.2.query.weight: 8192\n",
      "layers.5.attention.heads.2.key.weight: 8192\n",
      "layers.5.attention.heads.2.value.weight: 8192\n",
      "layers.5.attention.heads.3.query.weight: 8192\n",
      "layers.5.attention.heads.3.key.weight: 8192\n",
      "layers.5.attention.heads.3.value.weight: 8192\n",
      "layers.5.attention.heads.4.query.weight: 8192\n",
      "layers.5.attention.heads.4.key.weight: 8192\n",
      "layers.5.attention.heads.4.value.weight: 8192\n",
      "layers.5.attention.heads.5.query.weight: 8192\n",
      "layers.5.attention.heads.5.key.weight: 8192\n",
      "layers.5.attention.heads.5.value.weight: 8192\n",
      "layers.5.attention.heads.6.query.weight: 8192\n",
      "layers.5.attention.heads.6.key.weight: 8192\n",
      "layers.5.attention.heads.6.value.weight: 8192\n",
      "layers.5.attention.heads.7.query.weight: 8192\n",
      "layers.5.attention.heads.7.key.weight: 8192\n",
      "layers.5.attention.heads.7.value.weight: 8192\n",
      "layers.6.attention.heads.0.query.weight: 8192\n",
      "layers.6.attention.heads.0.key.weight: 8192\n",
      "layers.6.attention.heads.0.value.weight: 8192\n",
      "layers.6.attention.heads.1.query.weight: 8192\n",
      "layers.6.attention.heads.1.key.weight: 8192\n",
      "layers.6.attention.heads.1.value.weight: 8192\n",
      "layers.6.attention.heads.2.query.weight: 8192\n",
      "layers.6.attention.heads.2.key.weight: 8192\n",
      "layers.6.attention.heads.2.value.weight: 8192\n",
      "layers.6.attention.heads.3.query.weight: 8192\n",
      "layers.6.attention.heads.3.key.weight: 8192\n",
      "layers.6.attention.heads.3.value.weight: 8192\n",
      "layers.6.attention.heads.4.query.weight: 8192\n",
      "layers.6.attention.heads.4.key.weight: 8192\n",
      "layers.6.attention.heads.4.value.weight: 8192\n",
      "layers.6.attention.heads.5.query.weight: 8192\n",
      "layers.6.attention.heads.5.key.weight: 8192\n",
      "layers.6.attention.heads.5.value.weight: 8192\n",
      "layers.6.attention.heads.6.query.weight: 8192\n",
      "layers.6.attention.heads.6.key.weight: 8192\n",
      "layers.6.attention.heads.6.value.weight: 8192\n",
      "layers.6.attention.heads.7.query.weight: 8192\n",
      "layers.6.attention.heads.7.key.weight: 8192\n",
      "layers.6.attention.heads.7.value.weight: 8192\n",
      "layers.7.attention.heads.0.query.weight: 8192\n",
      "layers.7.attention.heads.0.key.weight: 8192\n",
      "layers.7.attention.heads.0.value.weight: 8192\n",
      "layers.7.attention.heads.1.query.weight: 8192\n",
      "layers.7.attention.heads.1.key.weight: 8192\n",
      "layers.7.attention.heads.1.value.weight: 8192\n",
      "layers.7.attention.heads.2.query.weight: 8192\n",
      "layers.7.attention.heads.2.key.weight: 8192\n",
      "layers.7.attention.heads.2.value.weight: 8192\n",
      "layers.7.attention.heads.3.query.weight: 8192\n",
      "layers.7.attention.heads.3.key.weight: 8192\n",
      "layers.7.attention.heads.3.value.weight: 8192\n",
      "layers.7.attention.heads.4.query.weight: 8192\n",
      "layers.7.attention.heads.4.key.weight: 8192\n",
      "layers.7.attention.heads.4.value.weight: 8192\n",
      "layers.7.attention.heads.5.query.weight: 8192\n",
      "layers.7.attention.heads.5.key.weight: 8192\n",
      "layers.7.attention.heads.5.value.weight: 8192\n",
      "layers.7.attention.heads.6.query.weight: 8192\n",
      "layers.7.attention.heads.6.key.weight: 8192\n",
      "layers.7.attention.heads.6.value.weight: 8192\n",
      "layers.7.attention.heads.7.query.weight: 8192\n",
      "layers.7.attention.heads.7.key.weight: 8192\n",
      "layers.7.attention.heads.7.value.weight: 8192\n",
      "layers.8.attention.heads.0.query.weight: 8192\n",
      "layers.8.attention.heads.0.key.weight: 8192\n",
      "layers.8.attention.heads.0.value.weight: 8192\n",
      "layers.8.attention.heads.1.query.weight: 8192\n",
      "layers.8.attention.heads.1.key.weight: 8192\n",
      "layers.8.attention.heads.1.value.weight: 8192\n",
      "layers.8.attention.heads.2.query.weight: 8192\n",
      "layers.8.attention.heads.2.key.weight: 8192\n",
      "layers.8.attention.heads.2.value.weight: 8192\n",
      "layers.8.attention.heads.3.query.weight: 8192\n",
      "layers.8.attention.heads.3.key.weight: 8192\n",
      "layers.8.attention.heads.3.value.weight: 8192\n",
      "layers.8.attention.heads.4.query.weight: 8192\n",
      "layers.8.attention.heads.4.key.weight: 8192\n",
      "layers.8.attention.heads.4.value.weight: 8192\n",
      "layers.8.attention.heads.5.query.weight: 8192\n",
      "layers.8.attention.heads.5.key.weight: 8192\n",
      "layers.8.attention.heads.5.value.weight: 8192\n",
      "layers.8.attention.heads.6.query.weight: 8192\n",
      "layers.8.attention.heads.6.key.weight: 8192\n",
      "layers.8.attention.heads.6.value.weight: 8192\n",
      "layers.8.attention.heads.7.query.weight: 8192\n",
      "layers.8.attention.heads.7.key.weight: 8192\n",
      "layers.8.attention.heads.7.value.weight: 8192\n",
      "layers.9.attention.heads.0.query.weight: 8192\n",
      "layers.9.attention.heads.0.key.weight: 8192\n",
      "layers.9.attention.heads.0.value.weight: 8192\n",
      "layers.9.attention.heads.1.query.weight: 8192\n",
      "layers.9.attention.heads.1.key.weight: 8192\n",
      "layers.9.attention.heads.1.value.weight: 8192\n",
      "layers.9.attention.heads.2.query.weight: 8192\n",
      "layers.9.attention.heads.2.key.weight: 8192\n",
      "layers.9.attention.heads.2.value.weight: 8192\n",
      "layers.9.attention.heads.3.query.weight: 8192\n",
      "layers.9.attention.heads.3.key.weight: 8192\n",
      "layers.9.attention.heads.3.value.weight: 8192\n",
      "layers.9.attention.heads.4.query.weight: 8192\n",
      "layers.9.attention.heads.4.key.weight: 8192\n",
      "layers.9.attention.heads.4.value.weight: 8192\n",
      "layers.9.attention.heads.5.query.weight: 8192\n",
      "layers.9.attention.heads.5.key.weight: 8192\n",
      "layers.9.attention.heads.5.value.weight: 8192\n",
      "layers.9.attention.heads.6.query.weight: 8192\n",
      "layers.9.attention.heads.6.key.weight: 8192\n",
      "layers.9.attention.heads.6.value.weight: 8192\n",
      "layers.9.attention.heads.7.query.weight: 8192\n",
      "layers.9.attention.heads.7.key.weight: 8192\n",
      "layers.9.attention.heads.7.value.weight: 8192\n",
      "layers.10.attention.heads.0.query.weight: 8192\n",
      "layers.10.attention.heads.0.key.weight: 8192\n",
      "layers.10.attention.heads.0.value.weight: 8192\n",
      "layers.10.attention.heads.1.query.weight: 8192\n",
      "layers.10.attention.heads.1.key.weight: 8192\n",
      "layers.10.attention.heads.1.value.weight: 8192\n",
      "layers.10.attention.heads.2.query.weight: 8192\n",
      "layers.10.attention.heads.2.key.weight: 8192\n",
      "layers.10.attention.heads.2.value.weight: 8192\n",
      "layers.10.attention.heads.3.query.weight: 8192\n",
      "layers.10.attention.heads.3.key.weight: 8192\n",
      "layers.10.attention.heads.3.value.weight: 8192\n",
      "layers.10.attention.heads.4.query.weight: 8192\n",
      "layers.10.attention.heads.4.key.weight: 8192\n",
      "layers.10.attention.heads.4.value.weight: 8192\n",
      "layers.10.attention.heads.5.query.weight: 8192\n",
      "layers.10.attention.heads.5.key.weight: 8192\n",
      "layers.10.attention.heads.5.value.weight: 8192\n",
      "layers.10.attention.heads.6.query.weight: 8192\n",
      "layers.10.attention.heads.6.key.weight: 8192\n",
      "layers.10.attention.heads.6.value.weight: 8192\n",
      "layers.10.attention.heads.7.query.weight: 8192\n",
      "layers.10.attention.heads.7.key.weight: 8192\n",
      "layers.10.attention.heads.7.value.weight: 8192\n",
      "layers.11.attention.heads.0.query.weight: 8192\n",
      "layers.11.attention.heads.0.key.weight: 8192\n",
      "layers.11.attention.heads.0.value.weight: 8192\n",
      "layers.11.attention.heads.1.query.weight: 8192\n",
      "layers.11.attention.heads.1.key.weight: 8192\n",
      "layers.11.attention.heads.1.value.weight: 8192\n",
      "layers.11.attention.heads.2.query.weight: 8192\n",
      "layers.11.attention.heads.2.key.weight: 8192\n",
      "layers.11.attention.heads.2.value.weight: 8192\n",
      "layers.11.attention.heads.3.query.weight: 8192\n",
      "layers.11.attention.heads.3.key.weight: 8192\n",
      "layers.11.attention.heads.3.value.weight: 8192\n",
      "layers.11.attention.heads.4.query.weight: 8192\n",
      "layers.11.attention.heads.4.key.weight: 8192\n",
      "layers.11.attention.heads.4.value.weight: 8192\n",
      "layers.11.attention.heads.5.query.weight: 8192\n",
      "layers.11.attention.heads.5.key.weight: 8192\n",
      "layers.11.attention.heads.5.value.weight: 8192\n",
      "layers.11.attention.heads.6.query.weight: 8192\n",
      "layers.11.attention.heads.6.key.weight: 8192\n",
      "layers.11.attention.heads.6.value.weight: 8192\n",
      "layers.11.attention.heads.7.query.weight: 8192\n",
      "layers.11.attention.heads.7.key.weight: 8192\n",
      "layers.11.attention.heads.7.value.weight: 8192\n",
      "block.0.attention.heads.0.query.weight: 8192\n",
      "block.0.attention.heads.0.key.weight: 8192\n",
      "block.0.attention.heads.0.value.weight: 8192\n",
      "block.0.attention.heads.1.query.weight: 8192\n",
      "block.0.attention.heads.1.key.weight: 8192\n",
      "block.0.attention.heads.1.value.weight: 8192\n",
      "block.0.attention.heads.2.query.weight: 8192\n",
      "block.0.attention.heads.2.key.weight: 8192\n",
      "block.0.attention.heads.2.value.weight: 8192\n",
      "block.0.attention.heads.3.query.weight: 8192\n",
      "block.0.attention.heads.3.key.weight: 8192\n",
      "block.0.attention.heads.3.value.weight: 8192\n",
      "block.0.attention.heads.4.query.weight: 8192\n",
      "block.0.attention.heads.4.key.weight: 8192\n",
      "block.0.attention.heads.4.value.weight: 8192\n",
      "block.0.attention.heads.5.query.weight: 8192\n",
      "block.0.attention.heads.5.key.weight: 8192\n",
      "block.0.attention.heads.5.value.weight: 8192\n",
      "block.0.attention.heads.6.query.weight: 8192\n",
      "block.0.attention.heads.6.key.weight: 8192\n",
      "block.0.attention.heads.6.value.weight: 8192\n",
      "block.0.attention.heads.7.query.weight: 8192\n",
      "block.0.attention.heads.7.key.weight: 8192\n",
      "block.0.attention.heads.7.value.weight: 8192\n",
      "layers.0.ff.net.0.bias: 768\n",
      "layers.1.ff.net.0.bias: 768\n",
      "layers.2.ff.net.0.bias: 768\n",
      "layers.3.ff.net.0.bias: 768\n",
      "layers.4.ff.net.0.bias: 768\n",
      "layers.5.ff.net.0.bias: 768\n",
      "layers.6.ff.net.0.bias: 768\n",
      "layers.7.ff.net.0.bias: 768\n",
      "layers.8.ff.net.0.bias: 768\n",
      "layers.9.ff.net.0.bias: 768\n",
      "layers.10.ff.net.0.bias: 768\n",
      "layers.11.ff.net.0.bias: 768\n",
      "block.0.ff.net.0.bias: 768\n",
      "layers.0.attention.combine_heads.bias: 256\n",
      "layers.0.ff.net.2.bias: 256\n",
      "layers.0.norm1.gamma: 256\n",
      "layers.0.norm1.beta: 256\n",
      "layers.0.norm2.gamma: 256\n",
      "layers.0.norm2.beta: 256\n",
      "layers.1.attention.combine_heads.bias: 256\n",
      "layers.1.ff.net.2.bias: 256\n",
      "layers.1.norm1.gamma: 256\n",
      "layers.1.norm1.beta: 256\n",
      "layers.1.norm2.gamma: 256\n",
      "layers.1.norm2.beta: 256\n",
      "layers.2.attention.combine_heads.bias: 256\n",
      "layers.2.ff.net.2.bias: 256\n",
      "layers.2.norm1.gamma: 256\n",
      "layers.2.norm1.beta: 256\n",
      "layers.2.norm2.gamma: 256\n",
      "layers.2.norm2.beta: 256\n",
      "layers.3.attention.combine_heads.bias: 256\n",
      "layers.3.ff.net.2.bias: 256\n",
      "layers.3.norm1.gamma: 256\n",
      "layers.3.norm1.beta: 256\n",
      "layers.3.norm2.gamma: 256\n",
      "layers.3.norm2.beta: 256\n",
      "layers.4.attention.combine_heads.bias: 256\n",
      "layers.4.ff.net.2.bias: 256\n",
      "layers.4.norm1.gamma: 256\n",
      "layers.4.norm1.beta: 256\n",
      "layers.4.norm2.gamma: 256\n",
      "layers.4.norm2.beta: 256\n",
      "layers.5.attention.combine_heads.bias: 256\n",
      "layers.5.ff.net.2.bias: 256\n",
      "layers.5.norm1.gamma: 256\n",
      "layers.5.norm1.beta: 256\n",
      "layers.5.norm2.gamma: 256\n",
      "layers.5.norm2.beta: 256\n",
      "layers.6.attention.combine_heads.bias: 256\n",
      "layers.6.ff.net.2.bias: 256\n",
      "layers.6.norm1.gamma: 256\n",
      "layers.6.norm1.beta: 256\n",
      "layers.6.norm2.gamma: 256\n",
      "layers.6.norm2.beta: 256\n",
      "layers.7.attention.combine_heads.bias: 256\n",
      "layers.7.ff.net.2.bias: 256\n",
      "layers.7.norm1.gamma: 256\n",
      "layers.7.norm1.beta: 256\n",
      "layers.7.norm2.gamma: 256\n",
      "layers.7.norm2.beta: 256\n",
      "layers.8.attention.combine_heads.bias: 256\n",
      "layers.8.ff.net.2.bias: 256\n",
      "layers.8.norm1.gamma: 256\n",
      "layers.8.norm1.beta: 256\n",
      "layers.8.norm2.gamma: 256\n",
      "layers.8.norm2.beta: 256\n",
      "layers.9.attention.combine_heads.bias: 256\n",
      "layers.9.ff.net.2.bias: 256\n",
      "layers.9.norm1.gamma: 256\n",
      "layers.9.norm1.beta: 256\n",
      "layers.9.norm2.gamma: 256\n",
      "layers.9.norm2.beta: 256\n",
      "layers.10.attention.combine_heads.bias: 256\n",
      "layers.10.ff.net.2.bias: 256\n",
      "layers.10.norm1.gamma: 256\n",
      "layers.10.norm1.beta: 256\n",
      "layers.10.norm2.gamma: 256\n",
      "layers.10.norm2.beta: 256\n",
      "layers.11.attention.combine_heads.bias: 256\n",
      "layers.11.ff.net.2.bias: 256\n",
      "layers.11.norm1.gamma: 256\n",
      "layers.11.norm1.beta: 256\n",
      "layers.11.norm2.gamma: 256\n",
      "layers.11.norm2.beta: 256\n",
      "block.0.attention.combine_heads.bias: 256\n",
      "block.0.ff.net.2.bias: 256\n",
      "block.0.norm1.gamma: 256\n",
      "block.0.norm1.beta: 256\n",
      "block.0.norm2.gamma: 256\n",
      "block.0.norm2.beta: 256\n"
     ]
    }
   ],
   "source": [
    "# get the number of parameters\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "parameter_to_data_ratio = n_params / len(train_data)\n",
    "print(f\"{parameter_to_data_ratio=}\")\n",
    "\n",
    "parameters = []\n",
    "for name, param in model.named_parameters():\n",
    "    parameters.append({\"name\": name, \"params\": param.numel()})\n",
    "\n",
    "# sort parameters by size\n",
    "sorted_parameters = sorted(parameters, key=lambda x: x[\"params\"], reverse=True)\n",
    "for p in sorted_parameters:\n",
    "    print(f\"{p['name']}: {p['params']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/60000 [00:00<6:06:36,  2.73it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.37523791193962097, 'val': 0.3870525062084198, 'l2': tensor(0.2279, device='cuda:0', grad_fn=<DivBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 302/60000 [00:35<4:06:06,  4.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.38546469807624817, 'val': 0.38315415382385254, 'l2': tensor(0.2282, device='cuda:0', grad_fn=<DivBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 602/60000 [01:11<3:58:11,  4.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.38080012798309326, 'val': 0.3887944221496582, 'l2': tensor(0.2284, device='cuda:0', grad_fn=<DivBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 902/60000 [01:46<3:59:02,  4.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.3815082609653473, 'val': 0.38819020986557007, 'l2': tensor(0.2287, device='cuda:0', grad_fn=<DivBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1202/60000 [02:21<3:54:00,  4.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.38699987530708313, 'val': 0.3811870515346527, 'l2': tensor(0.2290, device='cuda:0', grad_fn=<DivBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1502/60000 [02:57<3:53:55,  4.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.3874744474887848, 'val': 0.37942078709602356, 'l2': tensor(0.2292, device='cuda:0', grad_fn=<DivBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1802/60000 [03:32<3:56:17,  4.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.37514081597328186, 'val': 0.38777804374694824, 'l2': tensor(0.2295, device='cuda:0', grad_fn=<DivBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 2102/60000 [04:07<3:48:53,  4.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.38220518827438354, 'val': 0.38456523418426514, 'l2': tensor(0.2297, device='cuda:0', grad_fn=<DivBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2402/60000 [04:43<3:49:41,  4.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.38074561953544617, 'val': 0.3859330713748932, 'l2': tensor(0.2300, device='cuda:0', grad_fn=<DivBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 2702/60000 [05:18<3:47:45,  4.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.37715062499046326, 'val': 0.38477545976638794, 'l2': tensor(0.2303, device='cuda:0', grad_fn=<DivBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 3002/60000 [05:53<3:47:52,  4.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.3810678720474243, 'val': 0.386946439743042, 'l2': tensor(0.2305, device='cuda:0', grad_fn=<DivBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 3231/60000 [06:19<1:51:09,  8.51it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m l2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters()) \u001b[38;5;241m/\u001b[39m num_params\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m+\u001b[39m l2 \u001b[38;5;241m*\u001b[39m l2_penalty\n\u001b[0;32m---> 13\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m steps \u001b[38;5;241m%\u001b[39m eval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.12/site-packages/torch/_tensor.py:513\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the gradient of current tensor wrt graph leaves.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \n\u001b[1;32m    468\u001b[0m \u001b[38;5;124;03mThe graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;124;03m        used to compute the attr::tensors.\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.12/site-packages/torch/overrides.py:1604\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;66;03m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[1;32m   1602\u001b[0m     \u001b[38;5;66;03m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[1;32m   1603\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _pop_mode_temporarily() \u001b[38;5;28;01mas\u001b[39;00m mode:\n\u001b[0;32m-> 1604\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mmode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1605\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1606\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.12/site-packages/torch/utils/_device.py:77\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "num_params = sum([p.numel() for p in model.parameters()])\n",
    "\n",
    "for steps in tqdm.tqdm(range(max_iters)):\n",
    "    xb, yb = get_batch('train')\n",
    "    # loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # l2 regularization\n",
    "    l2 = sum(p.pow(2).sum() for p in model.parameters()) / num_params\n",
    "    loss = loss + l2 * l2_penalty\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        # wandb.log({\"tIain\": losses['train'].item(), \"val\": losses['val'].item(), \"l2\":l2})\n",
    "        print({\"tIain\": losses['train'].item(), \"val\": losses['val'].item(), \"l2\":l2})\n",
    "\n",
    "losses = estimate_loss(is_last=True)\n",
    "wandb.log({\"train\": losses['train'].item(), \"val\": losses['val'].item()})\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': tensor(0.4513, device='cuda:0'),\n",
       " 'val': tensor(0.4492, device='cuda:0')}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), 'tiny-stories-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model\n",
    "model.load_state_dict(torch.load('tiny-stories-model.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[198]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max saw a bear in the woods. He was scared and ran away. Max was happy that he was safe. He went back to the tree and played with his friends. They all had a fun day together.\"\n",
      "\"Once upon a time, there was a little girl named Lily. She loved to play with her toys and her friends. One day, she went to the park with her mom. They saw a big slide and Lily wanted to go on it.\n",
      "\n",
      "Lily's mom said, \"\"Be careful, Lily. You might fall and hurt yourself.\"\" Lily said, \"\"I will be careful, Mom.\"\" She climbed up the ladder and slid down the slide. It was so much fun!\n",
      "\n",
      "After playing on the slide, Lily and her mom went home. Lily was tired but happy. She told her mom, \"\"I love the park, Mom. It's so pretty!\"\" Her mom smiled and said, \"\"I'm glad you had fun, Lily.\"\"\"\n",
      "\"Once upon a time, there was a little\n"
     ]
    }
   ],
   "source": [
    "print(model.prompt_model(\"Max saw a bear in the woods.\", 200, 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "The first room was filled with joy. He raised wide and wide-bye and skipped on to the chest with a big smile. Every nowe on the same head. Aneagull was the most beautiful and quiet place! Aeon lilyolog opened the window andœOf course clapped in the lush road. Forward was nothing! Today had been great! She was so happy and so excited that she cried out that they couldn't stop until they gave up. She had done it!\"\n",
      "\"Once upon a time there was an angry volcano. Daisy shouted loud and refused to do anything.\n",
      "\n",
      "Then, bowl was filled with her anger from the volcano. She wished that the volcano would be more careful.\n",
      "\n",
      "But, the volcano didn't ignorant. Gootators were too bossy. It wanted to cause trouble and become angry.\"\n",
      "\"Once upon a time, there was a man who wanted to go for a ride. He was so excited! â€œThomas!â€ he shouted. â€œLet's go!â€\n",
      "\n",
      "Thomas and the man got on the bus. The billboard opened. Out of the window light came on Max but he got so excited. The picture in the picture was there all about him.\n"
     ]
    }
   ],
   "source": [
    "test_idx = torch.zeros(1, T).long() * 198\n",
    "print(decode(\n",
    "    model.generate(idx=test_idx, max_new_tokens=C)[0].tolist()\n",
    ")[T:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
